{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "# print(torch.version)\n",
    "# print(torch.version.cuda)\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "# loading all packages here to start\n",
    "from eval_patch_features.logistic import eval_linear\n",
    "from eval_patch_features.ann import eval_ANN\n",
    "from eval_patch_features.knn import eval_knn\n",
    "from eval_patch_features.protonet import eval_protonet\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "VECTOR_DIM = 1024\n",
    "CLUSTERING_METHOD = 'kmeans'\n",
    "NUM_CLUSTERS = 3\n",
    "NUM_PATCHES_PER_CLUSTER = 0\n",
    "BATCH_SIZE = 32\n",
    "K_FOLDS_PATH = r\"E:\\KSA Project\\dataset\\splits\\kfolds.csv\"\n",
    "DATA_PATH = r\"E:\\KSA Project\\dataset\\uni_fivecrop_features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from typing import List, Tuple\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class WSIDataset(Dataset):\n",
    "    def __init__(self, save_dir: str, fold_ids: List[str]):\n",
    "        self.data = []\n",
    "        self.save_dir = save_dir\n",
    "        self.fold_ids = fold_ids\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        for wsi_folder in os.listdir(self.save_dir):\n",
    "            wsi_folder_path = os.path.join(self.save_dir, wsi_folder)\n",
    "            if not os.path.isdir(wsi_folder_path) and len(os.listdir(wsi_folder_path)) <= 15:\n",
    "                # print(f\"Skipping {wsi_folder} due to less than 18 patches\")\n",
    "                continue\n",
    "            for wsi_file in os.listdir(wsi_folder_path):\n",
    "                if wsi_file.endswith('.pt'):\n",
    "                    wsi_id = wsi_file[:12]\n",
    "                    if wsi_id not in self.fold_ids:\n",
    "                        continue\n",
    "                    try:\n",
    "                        wsi_features = torch.load(os.path.join(wsi_folder_path, wsi_file))\n",
    "                        # check if loaded features is not one feature vector then average them to make one feature vector\n",
    "                        if isinstance(wsi_features, torch.Tensor) and wsi_features.dim() > 1:\n",
    "                            # print(f\"WSI ID: {wsi_id} | Features Shape: {wsi_features.shape}\")\n",
    "                            wsi_features = torch.mean(wsi_features, dim=0)\n",
    "                        label = 0 if '_nonMSI' in wsi_file else 1\n",
    "                        self.data.append((wsi_features, label, wsi_id))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading {os.path.join(wsi_folder_path, wsi_file)}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label, wsi_id = self.data[idx]\n",
    "        return features, label, wsi_id\n",
    "    \n",
    "    def apply_clustering(self, clustering_algorithm: str, num_clusters: int = 3, num_selected_patches: int = 0):\n",
    "        \"\"\"\n",
    "        Apply clustering on the patches of each WSI and create a consistent WSI representation.\n",
    "        Args:\n",
    "        - clustering_algorithm: The clustering algorithm to use ('kmeans', 'dbscan', 'pca').\n",
    "        - num_clusters: Number of clusters to create (only for k-means or similar algorithms).\n",
    "        - num_selected_patches: Number of top patches to use for averaging within each cluster (optional).\n",
    "        \"\"\"\n",
    "        clustered_data = []\n",
    "        wsi_ids = set([wsi_id for _, _, wsi_id in self.data])  # Unique WSI IDs\n",
    "\n",
    "        for wsi_id in wsi_ids:\n",
    "            # Extract all patches for the WSI\n",
    "            wsi_patches = [features for features, _, id in self.data if id == wsi_id]\n",
    "            wsi_patches = torch.stack(wsi_patches)\n",
    "            patch_array = wsi_patches.numpy()  # Convert to numpy for clustering\n",
    "\n",
    "            # Step 1: Perform clustering\n",
    "            if clustering_algorithm == 'kmeans':\n",
    "                clustering_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "                clustering_model.fit(patch_array)\n",
    "                cluster_labels = clustering_model.labels_\n",
    "                cluster_centroids = clustering_model.cluster_centers_\n",
    "            elif clustering_algorithm == 'dbscan':\n",
    "                clustering_model = DBSCAN(eps=0.1, min_samples=2)\n",
    "                cluster_labels = clustering_model.fit_predict(patch_array)\n",
    "                unique_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "                num_clusters = unique_clusters\n",
    "                # print(f\"Unique clusters: {unique_clusters}\")\n",
    "                cluster_centroids = np.array([np.mean(patch_array[cluster_labels == i], axis=0) for i in range(num_clusters)])\n",
    "                \n",
    "            elif clustering_algorithm == 'pca':\n",
    "                pca = PCA(n_components=num_clusters)\n",
    "                transformed_features = pca.fit_transform(patch_array)\n",
    "                cluster_labels = np.argmax(transformed_features, axis=1)\n",
    "                cluster_centroids = pca.components_\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported clustering algorithm: {clustering_algorithm}\")\n",
    "\n",
    "            # Step 2: Aggregate features within each cluster\n",
    "            selected_features = []\n",
    "            cluster_sums = []\n",
    "            for cluster_idx in range(num_clusters):\n",
    "                cluster_patches = patch_array[cluster_labels == cluster_idx]\n",
    "                if len(cluster_patches) == 0:\n",
    "                    continue\n",
    "                # Optionally select top-ranked patches\n",
    "                if num_selected_patches > 0:\n",
    "                    distances = cdist(cluster_patches, [cluster_centroids[cluster_idx]], metric='euclidean').flatten()\n",
    "                    sorted_indices = np.argsort(distances)\n",
    "                    cluster_patches = cluster_patches[sorted_indices[:num_selected_patches]]\n",
    "\n",
    "                # Average the cluster features and calculate cluster sum for sorting\n",
    "                cluster_average = np.mean(cluster_patches, axis=0)\n",
    "                cluster_sum = np.average(cluster_average)\n",
    "                selected_features.append(cluster_average)\n",
    "                cluster_sums.append(cluster_sum)\n",
    "\n",
    "            # Step 3: Sort clusters by their sum values\n",
    "            sorted_indices = np.argsort(cluster_sums)\n",
    "            sorted_features = np.array(selected_features)[sorted_indices]\n",
    "\n",
    "            # Step 4: Create a consistent WSI-level representation\n",
    "            concatenated_features = np.concatenate(sorted_features)\n",
    "            label = [label for _, label, id in self.data if id == wsi_id][0]  # Assume all patches have the same label\n",
    "            clustered_data.append((torch.tensor(concatenated_features), label, wsi_id))\n",
    "\n",
    "            # Print debug information (optional)\n",
    "            # print(f\"WSI ID: {wsi_id} | Total Patches: {len(patch_array)} | Clusters: {num_clusters} | Sorted Features Shape: {concatenated_features.shape}\")\n",
    "\n",
    "        # Update the dataset with clustered data\n",
    "        self.data = clustered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_averages_by_index(all_fold_results, metric_indices):\n",
    "    \"\"\"\n",
    "    Calculate the average of specified metrics over multiple folds.\n",
    "\n",
    "    Args:\n",
    "        all_fold_results (list of dicts): Results for each fold.\n",
    "        metric_indices (dict): Mapping of metric names to their indices.\n",
    "\n",
    "    Returns:\n",
    "        dict: Averages of the specified metrics across folds.\n",
    "    \"\"\"\n",
    "    # Initialize averages dictionary\n",
    "    averages = {metric: 0 for metric in metric_indices.keys()}\n",
    "    counts = {metric: 0 for metric in metric_indices.keys()}  # Keep track of valid metrics\n",
    "    num_folds = len(all_fold_results)\n",
    "\n",
    "    for result in all_fold_results:\n",
    "        # Iterate through metrics by their index\n",
    "        for metric, index in metric_indices.items():\n",
    "            try:\n",
    "                metric_name = list(result.keys())[index]  # Extract the metric name by index\n",
    "                if metric_name in result and isinstance(result[metric_name], (int, float)):  # Check if metric exists and is numeric\n",
    "                    averages[metric] += result[metric_name]\n",
    "                    counts[metric] += 1\n",
    "            except IndexError:\n",
    "                # Metric not present in this result due to model differences\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing metric '{metric}': {e}\")\n",
    "    # Compute average only for metrics with valid values\n",
    "    for metric in averages:\n",
    "        if counts[metric] > 0:\n",
    "            averages[metric] /= counts[metric]\n",
    "    return averages\n",
    "\n",
    " \n",
    "\n",
    "def print_metrics(metrics):\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, (int, float)):  # Check if the value is a number\n",
    "            print(f\"{key}: {value:.4f}\")  # Format numbers with 4 decimal places\n",
    "        # else:\n",
    "        #     print(f\"{key}: {value}\")  # For non-numeric values, just print them directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_loader,val_loader, test_loader, model_type='linear'):\n",
    "    all_train_feats, all_train_labels,all_val_feats,all_val_labels, all_test_feats, all_test_labels = [], [], [], [], [], []\n",
    "    all_test_ids = []\n",
    "    \n",
    "    # Prepare training and testing data\n",
    "    for features, label, _ in train_loader:\n",
    "        all_train_feats.append(features)\n",
    "        all_train_labels.append(label)\n",
    "    for features, label, _ in val_loader:\n",
    "        all_val_feats.append(features)\n",
    "        all_val_labels.append(label)\n",
    "    for features, label, wsi_id in test_loader:\n",
    "        all_test_feats.append(features)\n",
    "        all_test_labels.append(label)\n",
    "        all_test_ids.append(wsi_id)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    global train_feats, train_labels, val_feats, val_labels, test_feats, test_labels\n",
    "    train_feats = torch.cat(all_train_feats)\n",
    "    train_labels = torch.cat([labels.clone().detach() for labels in all_train_labels])\n",
    "    val_feats = torch.cat(all_val_feats)\n",
    "    val_labels = torch.cat([labels.clone().detach() for labels in all_val_labels])\n",
    "    test_feats = torch.cat(all_test_feats)\n",
    "    test_labels = torch.cat([labels.clone().detach() for labels in all_test_labels])\n",
    "    \n",
    "    # Select the model based on the input argument\n",
    "    if model_type == 'linear':\n",
    "        eval_metrics, eval_dump = eval_linear(\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            valid_feats=val_feats,  # Optionally, use a separate validation set\n",
    "            valid_labels=val_labels,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            max_iter=250,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif model_type == 'ann':\n",
    "        eval_metrics, eval_dump = eval_ANN(\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            valid_feats=val_feats,\n",
    "            valid_labels=val_labels,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            combine_trainval=False,\n",
    "            input_dim=VECTOR_DIM * NUM_CLUSTERS,\n",
    "            max_iter=250,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif model_type == 'knn':\n",
    "        eval_metrics, eval_dump = eval_knn(\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            val_feats=val_feats,\n",
    "            val_labels=val_labels,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            n_neighbors=5,\n",
    "            normalize_feats=True,\n",
    "            verbose=False\n",
    "        )\n",
    "    elif model_type == 'protonet':\n",
    "        eval_metrics, eval_dump = eval_protonet(\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            val_feats=val_feats,\n",
    "            val_labels=val_labels,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            normalize_feats=True,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    return eval_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_classes(dataset):\n",
    "    \"\"\"\n",
    "    Helper function to count class occurrences in a dataset.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    wsi_count = Counter([label for _, label, wsi_id in dataset.data])\n",
    "    for _, label, _ in DataLoader(dataset, batch_size=1, shuffle=False):\n",
    "        labels.append(label.item() if isinstance(label, torch.Tensor) else label)\n",
    "    return wsi_count, Counter(labels)\n",
    "\n",
    "# Cross-validation function\n",
    "def run_k_fold_cross_validation(save_dir: str, folds: List[List[str]], model_type: str = 'linear'):\n",
    "    results_per_fold = []\n",
    "\n",
    "    num_folds = len(folds)\n",
    "\n",
    "    for i in range(num_folds):\n",
    "        # Define test and validation folds\n",
    "        test_ids = folds[i]\n",
    "        val_ids = folds[(i + 1) % num_folds]  # The next fold in sequence is used as validation\n",
    "\n",
    "        # Use remaining folds as training\n",
    "        train_ids = []\n",
    "        for j in range(num_folds):\n",
    "            if j != i and j != (i + 1) % num_folds:\n",
    "                train_ids.extend(folds[j])\n",
    "\n",
    "        # Create datasets and loaders\n",
    "        train_dataset = WSIDataset(save_dir, train_ids)\n",
    "        # train_dataset.apply_clustering(clustering_algorithm=CLUSTERING_METHOD, num_clusters=NUM_CLUSTERS, num_selected_patches=NUM_PATCHES_PER_CLUSTER)\n",
    "        val_dataset = WSIDataset(save_dir, val_ids)\n",
    "        # val_dataset.apply_clustering(clustering_algorithm=CLUSTERING_METHOD,num_clusters=NUM_CLUSTERS, num_selected_patches=NUM_PATCHES_PER_CLUSTER)\n",
    "        test_dataset = WSIDataset(save_dir, test_ids)\n",
    "        # test_dataset.apply_clustering(clustering_algorithm=CLUSTERING_METHOD,num_clusters=NUM_CLUSTERS, num_selected_patches=NUM_PATCHES_PER_CLUSTER)\n",
    "        # train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        # val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        # test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        # Train and evaluate\n",
    "        print(f\"Running Fold {i + 1} with model {model_type}...\")\n",
    "        print(f\"Train: {count_classes(train_dataset)}\")\n",
    "        print(f\"Validation: {count_classes(val_dataset)}\")\n",
    "        print(f\"Test: {count_classes(test_dataset)}\")\n",
    "        # eval_metrics = train_and_evaluate(train_loader, val_loader,test_loader, model_type=model_type)\n",
    "        # print_metrics(eval_metrics)\n",
    "        # results_per_fold.append(eval_metrics)\n",
    "\n",
    "    return results_per_fold \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "folds_df = pd.read_csv(K_FOLDS_PATH)\n",
    "# Define your folds\n",
    "fold1_ids = folds_df['Fold1'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "fold2_ids = folds_df['Fold2'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "fold3_ids = folds_df['Fold3'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "fold4_ids = folds_df['Fold4'].dropna().apply(lambda x: x[:12]).tolist()\n",
    "folds = [fold1_ids, fold2_ids, fold3_ids, fold4_ids]\n",
    "\n",
    "# Run k-fold cross-validation with different models\n",
    "model_types = ['linear','ann','knn','protonet']\n",
    "# model_types = ['ann','protonet']\n",
    "metric_indices = {\n",
    "    'acc': 0,          # 'lin_acc' corresponds to index 0\n",
    "    'bacc': 1,         # 'lin_bacc' corresponds to index 1\n",
    "    'macro_f1': 2,     # 'lin_macro_f1' corresponds to index 2\n",
    "    'weighted_f1': 3,  # 'lin_weighted_f1' corresponds to index 3\n",
    "    'auroc': 5         # 'lin_auroc' corresponds to index 4\n",
    "}\n",
    "\n",
    "for model in model_types:\n",
    "    print(f\"\\n\\n ********* Training with model: {model}********* \\n\\n\")\n",
    "    k_folds_results = run_k_fold_cross_validation(DATA_PATH, folds, model_type=model)\n",
    "    # average_results = calculate_metric_averages_by_index(k_folds_results, metric_indices)\n",
    "    print(\"\\n\\n Average results for all folds:\")\n",
    "    # for metric, value in average_results.items():\n",
    "    #     print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " ********* Training with model: linear********* \n",
    "\n",
    "\n",
    "Running Fold 1 with model linear...\n",
    "Confusion Matrix:\n",
    "[[14  0]\n",
    " [ 4  0]]\n",
    "lin_acc: 0.7778\n",
    "lin_bacc: 0.5000\n",
    "lin_kappa: 0.0000\n",
    "lin_weighted_f1: 0.6806\n",
    "lin_auroc: 0.5000\n",
    "Running Fold 2 with model linear...\n",
    "Confusion Matrix:\n",
    "[[14  0]\n",
    " [ 5  2]]\n",
    "lin_acc: 0.7619\n",
    "lin_bacc: 0.6429\n",
    "lin_kappa: 0.3478\n",
    "lin_weighted_f1: 0.7138\n",
    "lin_auroc: 0.8265\n",
    "Running Fold 3 with model linear...\n",
    "Confusion Matrix:\n",
    "[[15  1]\n",
    " [ 3  0]]\n",
    "lin_acc: 0.7895\n",
    "lin_bacc: 0.4688\n",
    "lin_kappa: -0.0857\n",
    "lin_weighted_f1: 0.7430\n",
    "lin_auroc: 0.6250\n",
    "Running Fold 4 with model linear...\n",
    "Confusion Matrix:\n",
    "[[15  0]\n",
    " [ 0  1]]\n",
    "lin_acc: 1.0000\n",
    "lin_bacc: 1.0000\n",
    "lin_kappa: 1.0000\n",
    "lin_weighted_f1: 1.0000\n",
    "lin_auroc: 1.0000\n",
    "\n",
    "\n",
    " Average results for all folds:\n",
    "acc: 0.8323\n",
    "bacc: 0.6529\n",
    "kappa: 0.3155\n",
    "weighted_f1: 0.7843\n",
    "auroc: 0.7379\n",
    "\n",
    "\n",
    " ********* Training with model: ann********* \n",
    "\n",
    "\n",
    "Running Fold 1 with model ann...\n",
    "Confusion Matrix:\n",
    "[[11  3]\n",
    " [ 1  3]]\n",
    "ann_acc: 0.7778\n",
    "ann_bacc: 0.7679\n",
    "ann_kappa: 0.4545\n",
    "ann_weighted_f1: 0.7915\n",
    "ann_auroc: 0.9107\n",
    "Running Fold 2 with model ann...\n",
    "Confusion Matrix:\n",
    "[[12  2]\n",
    " [ 2  5]]\n",
    "ann_acc: 0.8095\n",
    "ann_bacc: 0.7857\n",
    "ann_kappa: 0.5714\n",
    "ann_weighted_f1: 0.8095\n",
    "ann_auroc: 0.8980\n",
    "Running Fold 3 with model ann...\n",
    "Confusion Matrix:\n",
    "[[9 7]\n",
    " [2 1]]\n",
    "ann_acc: 0.5263\n",
    "ann_bacc: 0.4479\n",
    "ann_kappa: -0.0621\n",
    "ann_weighted_f1: 0.5901\n",
    "ann_auroc: 0.6250\n",
    "Running Fold 4 with model ann...\n",
    "Confusion Matrix:\n",
    "[[9 6]\n",
    " [0 1]]\n",
    "ann_acc: 0.6250\n",
    "ann_bacc: 0.8000\n",
    "ann_kappa: 0.1579\n",
    "ann_weighted_f1: 0.7188\n",
    "ann_auroc: 1.0000\n",
    "\n",
    "\n",
    " Average results for all folds:\n",
    "acc: 0.6847\n",
    "bacc: 0.7004\n",
    "kappa: 0.2804\n",
    "weighted_f1: 0.7275\n",
    "auroc: 0.8584\n",
    "\n",
    "\n",
    " ********* Training with model: knn********* \n",
    "\n",
    "\n",
    "Running Fold 1 with model knn...\n",
    "knn5_acc: 0.7778\n",
    "knn5_bacc: 0.5000\n",
    "knn5_kappa: 0.0000\n",
    "knn5_weighted_f1: 0.6806\n",
    "Running Fold 2 with model knn...\n",
    "knn5_acc: 0.6667\n",
    "knn5_bacc: 0.5000\n",
    "knn5_kappa: 0.0000\n",
    "knn5_weighted_f1: 0.5333\n",
    "Running Fold 3 with model knn...\n",
    "knn5_acc: 0.8421\n",
    "knn5_bacc: 0.5000\n",
    "knn5_kappa: 0.0000\n",
    "knn5_weighted_f1: 0.7699\n",
    "Running Fold 4 with model knn...\n",
    "knn5_acc: 0.8750\n",
    "knn5_bacc: 0.4667\n",
    "knn5_kappa: -0.0667\n",
    "knn5_weighted_f1: 0.8750\n",
    "\n",
    "\n",
    " Average results for all folds:\n",
    "acc: 0.7904\n",
    "bacc: 0.4917\n",
    "kappa: -0.0167\n",
    "weighted_f1: 0.7147\n",
    "auroc: 0.0000\n",
    "\n",
    "\n",
    " ********* Training with model: protonet********* \n",
    "\n",
    "\n",
    "Running Fold 1 with model protonet...\n",
    "proto_acc: 0.6667\n",
    "proto_bacc: 0.4286\n",
    "proto_kappa: -0.1739\n",
    "proto_weighted_f1: 0.6222\n",
    "Running Fold 2 with model protonet...\n",
    "proto_acc: 0.6190\n",
    "proto_bacc: 0.5714\n",
    "proto_kappa: 0.1429\n",
    "proto_weighted_f1: 0.6190\n",
    "Running Fold 3 with model protonet...\n",
    "proto_acc: 0.8421\n",
    "proto_bacc: 0.5000\n",
    "proto_kappa: 0.0000\n",
    "proto_weighted_f1: 0.7699\n",
    "Running Fold 4 with model protonet...\n",
    "proto_acc: 0.8750\n",
    "proto_bacc: 0.9333\n",
    "proto_kappa: 0.4483\n",
    "proto_weighted_f1: 0.9018\n",
    "\n",
    "\n",
    " Average results for all folds:\n",
    "acc: 0.7507\n",
    "bacc: 0.6083\n",
    "kappa: 0.1043\n",
    "weighted_f1: 0.7282\n",
    "auroc: 0.0000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
