{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from os.path import join as j_\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Lambda\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "# loading all packages here to start\n",
    "from eval_patch_features.logistic import eval_linear\n",
    "from eval_patch_features.ann import eval_ANN\n",
    "from eval_patch_features.knn import eval_knn\n",
    "from eval_patch_features.protonet import eval_protonet\n",
    "from eval_patch_features.metrics import get_eval_metrics, print_metrics\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "VECTOR_DIM = 1280  # size of input feature vector\n",
    "HIDDEN_DIM = 768   # size of ANN hidden layer\n",
    "BATCH_SIZE = 8\n",
    "K_FOLDS_PATH = r\"E:\\Aamir Gulzar\\dataset\\paip_data\\labels\\TrainTest_paip.csv\"\n",
    "DATA_PATH = r\"E:\\Aamir Gulzar\\dataset\\paip_data\\virchow2_Features\"\n",
    "MODEL_SAVE_PATH = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = r\"E:\\Aamir Gulzar\\dataset\\paip_data\\CAIMAN_FiveCrop_Features\"\n",
    "# data_save = r\"E:\\Aamir Gulzar\\dataset\\paip_data\\CAIMAN_Features\"\n",
    "# if not os.path.exists(data_save):\n",
    "#     os.makedirs(data_save, exist_ok=True)\n",
    "# # load the data\n",
    "# for wsi in os.listdir(data):\n",
    "#     wsi_data = []\n",
    "#     for patch in os.listdir(j_(data, wsi)):\n",
    "#         patch_data = torch.load(j_(data, wsi, patch))\n",
    "#         # check if the loaded feature vector is five crop then average it first then append to the wsi_data\n",
    "#         if patch_data.shape[0] > 1:\n",
    "#             patch_data = patch_data.mean(dim=0)\n",
    "#         wsi_data.append(patch_data)\n",
    "#     wsi_data = torch.stack(wsi_data).mean(dim=0)\n",
    "#     save_path = j_(data_save, wsi + \".pt\")\n",
    "#     torch.save(wsi_data, save_path)\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Define the path to the .pt file\n",
    "# file_path = r\"E:\\Aamir Gulzar\\dataset\\paip_data\\UNI_Features\\training_data_01_MSIH.pt\"\n",
    "\n",
    "# # Load the .pt file\n",
    "# try:\n",
    "#     data = torch.load(file_path)\n",
    "#     shape = torch.tensor(data).shape  # Get shape\n",
    "#     print(\"Shape of the data:\", shape)\n",
    "#     print(\"Values:\\n\", data)  # Print values\n",
    "# except Exception as e:\n",
    "#     print(\"Error loading file:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List\n",
    "\n",
    "class WSIDataset(Dataset):\n",
    "    def __init__(self, save_dir: str, fold_ids: List[str]):\n",
    "        self.data = []\n",
    "        self.save_dir = save_dir\n",
    "        self.fold_ids = fold_ids\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        for wsi_file in os.listdir(self.save_dir):\n",
    "            wsi_path = os.path.join(self.save_dir, wsi_file)\n",
    "            # Extract the WSI ID and truncate it to the first 12 characters\n",
    "            wsi_id = os.path.splitext(wsi_file)[0]   # Extract first 12 characters\n",
    "            if wsi_id not in self.fold_ids:\n",
    "                continue  # Skip if the WSI is not in the current fold\n",
    "\n",
    "            if wsi_path.endswith('.pt'):\n",
    "                try:\n",
    "                    # Load WSI features\n",
    "                    wsi_features = torch.load(wsi_path)\n",
    "                    if wsi_features.is_cuda:\n",
    "                        wsi_features = wsi_features.cpu()\n",
    "                    # Average all feature vectors in the .pt file\n",
    "                    if wsi_features.dim() > 1:\n",
    "                        averaged_features = torch.mean(wsi_features, dim=0)\n",
    "                    else:\n",
    "                        averaged_features = wsi_features  # In case it is already a single vector\n",
    "\n",
    "                    # Determine label based on WSI file name\n",
    "                    label = 0 if '_nonMSI' in wsi_file else 1\n",
    "\n",
    "                    # Append the averaged features, label, and WSI ID\n",
    "                    self.data.append((averaged_features, label, wsi_id))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {wsi_path}: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, label, wsi_id = self.data[idx]\n",
    "        return features, label, wsi_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_averages_by_index(all_fold_results, metric_indices):\n",
    "    \"\"\"\n",
    "    Calculate the average of specified metrics over multiple folds.\n",
    "    Args:\n",
    "        all_fold_results (list of dicts): Results for each fold.\n",
    "        metric_indices (dict): Mapping of metric names to their indices.\n",
    "    Returns:\n",
    "        dict: Averages of the specified metrics across folds.\n",
    "    \"\"\"\n",
    "    # Initialize averages dictionary\n",
    "    averages = {metric: 0 for metric in metric_indices.keys()}\n",
    "    counts = {metric: 0 for metric in metric_indices.keys()}  # Keep track of valid metrics\n",
    "    num_folds = len(all_fold_results)\n",
    "\n",
    "    for result in all_fold_results:\n",
    "        # Iterate through metrics by their index\n",
    "        for metric, index in metric_indices.items():\n",
    "            try:\n",
    "                metric_name = list(result.keys())[index]  # Extract the metric name by index\n",
    "                if metric_name in result and isinstance(result[metric_name], (int, float)):  # Check if metric exists and is numeric\n",
    "                    averages[metric] += result[metric_name]\n",
    "                    counts[metric] += 1\n",
    "            except IndexError:\n",
    "                # Metric not present in this result due to model differences\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing metric '{metric}': {e}\")\n",
    "    # Compute average only for metrics with valid values\n",
    "    for metric in averages:\n",
    "        if counts[metric] > 0:\n",
    "            averages[metric] /= counts[metric]\n",
    "    return averages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(fold,train_loader, test_loader, model_type='linear'):\n",
    "    all_train_feats, all_train_labels, all_test_feats, all_test_labels = [], [], [], []\n",
    "    \n",
    "    # Prepare training and testing data\n",
    "    for features, label, _ in train_loader:\n",
    "        all_train_feats.append(features)\n",
    "        all_train_labels.append(label)\n",
    "    for features, label, wsi_id in test_loader:\n",
    "        all_test_feats.append(features)\n",
    "        all_test_labels.append(label)\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    global train_feats, train_labels, val_feats, val_labels, test_feats, test_labels\n",
    "    train_feats = torch.cat(all_train_feats)\n",
    "    train_labels = torch.cat([labels.clone().detach() for labels in all_train_labels])\n",
    "    test_feats = torch.cat(all_test_feats)\n",
    "    test_labels = torch.cat([labels.clone().detach() for labels in all_test_labels])\n",
    "    \n",
    "    # Select the model based on the input argument\n",
    "    if model_type == 'linear':\n",
    "        eval_metrics, eval_dump = eval_linear(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            valid_feats=None,  # Optionally, use a separate validation set\n",
    "            valid_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            max_iter=350,\n",
    "            save_path = MODEL_SAVE_PATH,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif model_type == 'ann':\n",
    "        eval_metrics, eval_dump = eval_ANN(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            valid_feats=None,\n",
    "            valid_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            input_dim=VECTOR_DIM,\n",
    "            hidden_dim = HIDDEN_DIM,\n",
    "            model_save_path = MODEL_SAVE_PATH,\n",
    "            max_iter=350,\n",
    "            verbose=False,\n",
    "        )\n",
    "    elif model_type == 'knn':\n",
    "        eval_metrics, eval_dump = eval_knn(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            val_feats=None,\n",
    "            val_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            n_neighbors=5,\n",
    "            normalize_feats=True,\n",
    "            model_save_path = MODEL_SAVE_PATH,\n",
    "            verbose=False\n",
    "        )\n",
    "    elif model_type == 'protonet':\n",
    "        eval_metrics, eval_dump = eval_protonet(\n",
    "            fold=fold,\n",
    "            train_feats=train_feats,\n",
    "            train_labels=train_labels,\n",
    "            val_feats=None,\n",
    "            val_labels=None,\n",
    "            test_feats=test_feats,\n",
    "            test_labels=test_labels,\n",
    "            normalize_feats=True,\n",
    "            model_save_path = MODEL_SAVE_PATH\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    return eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_classes(dataset):\n",
    "    \"\"\"\n",
    "    Helper function to count class occurrences in a dataset.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    for _, label, _ in DataLoader(dataset, batch_size=1, shuffle=False):\n",
    "        labels.append(label.item() if isinstance(label, torch.Tensor) else label)\n",
    "    return Counter(labels)\n",
    "\n",
    "def run_k_fold_cross_validation(save_dir: str, folds: List[List[str]], model_type: str = 'linear'):\n",
    "    results_per_fold = []\n",
    "    wsi_level_accuracies = []\n",
    "    num_folds = len(folds)\n",
    "\n",
    "    for i in range(1):\n",
    "        # Define test and validation folds\n",
    "        train_ids = folds[i]\n",
    "        test_ids = folds[i + 1]  # The next fold in sequence is used as validation\n",
    "        print(f\"Running Fold {i + 1} with model {model_type}...\")\n",
    "        # Create datasets and loaders\n",
    "        train_dataset = WSIDataset(save_dir, train_ids)\n",
    "        test_dataset = WSIDataset(save_dir, test_ids)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        eval_metrics = train_and_evaluate(i,train_loader, test_loader, model_type=model_type)\n",
    "        print_metrics(eval_metrics)\n",
    "        results_per_fold.append(eval_metrics)\n",
    "\n",
    "    return results_per_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ********* Training with model: linear********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model linear...\n",
      "lin_acc: 0.6129\n",
      "lin_bacc: 0.5982\n",
      "lin_macro_f1: 0.5571\n",
      "lin_weighted_f1: 0.6433\n",
      "lin_auroc: 0.6310\n",
      "lin_conf_matrix: [[15  9]\n",
      " [ 3  4]]\n",
      "\n",
      "\n",
      " ********* Training with model: ann********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model ann...\n",
      "acc: 0.5806\n",
      "bacc: 0.6280\n",
      "macro_f1: 0.5507\n",
      "weighted_f1: 0.6143\n",
      "auroc: 0.6250\n",
      "conf_matrix: [[13 11]\n",
      " [ 2  5]]\n",
      "\n",
      "\n",
      " ********* Training with model: knn********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model knn...\n",
      "knn_acc: 0.7742\n",
      "knn_bacc: 0.6012\n",
      "knn_macro_f1: 0.6132\n",
      "knn_weighted_f1: 0.7500\n",
      "knn_auroc: 0.5595\n",
      "knn_conf_matrix: [[22  2]\n",
      " [ 5  2]]\n",
      "\n",
      "\n",
      " ********* Training with model: protonet********* \n",
      "\n",
      "\n",
      "Running Fold 1 with model protonet...\n",
      "proto_acc: 0.5484\n",
      "proto_bacc: 0.5060\n",
      "proto_macro_f1: 0.4833\n",
      "proto_weighted_f1: 0.5839\n",
      "proto_auroc: 0.5357\n",
      "proto_conf_matrix: [[14 10]\n",
      " [ 4  3]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "folds_df = pd.read_csv(K_FOLDS_PATH)\n",
    "# Define your folds\n",
    "fold1_ids = folds_df['Fold1'].dropna().tolist()\n",
    "fold2_ids = folds_df['Fold2'].dropna().tolist()\n",
    "folds = [fold1_ids, fold2_ids]\n",
    "# Run k-fold cross-validation with different models\n",
    "model_types = ['linear','ann','knn','protonet']\n",
    "metric_indices = {\n",
    "    'acc': 0,          # 'lin_acc' corresponds to index 0\n",
    "    'bacc': 1,         # 'lin_bacc' corresponds to index 1\n",
    "    'macro_f1': 2,        # 'lin_kappa' corresponds to index 2\n",
    "    'weighted_f1': 3,  # 'lin_weighted_f1' corresponds to index 3\n",
    "    'auroc': 4         # 'lin_auroc' corresponds to index 4\n",
    "}\n",
    "\n",
    "for model in model_types:\n",
    "    print(f\"\\n\\n ********* Training with model: {model}********* \\n\\n\")\n",
    "    k_folds_results = run_k_fold_cross_validation(DATA_PATH, folds, model_type=model)\n",
    "    # average_results = calculate_metric_averages_by_index(k_folds_results, metric_indices)\n",
    "    # print(\"\\n\\n Average results for all folds:\")\n",
    "    # for metric, value in average_results.items():\n",
    "    #     print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
