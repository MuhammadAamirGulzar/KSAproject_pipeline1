{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the finalised IDARS pipeline using the FiveCrop transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import auc, roc_curve,roc_auc_score, f1_score, precision_recall_curve, average_precision_score, classification_report \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "from scipy.stats.mstats import gmean\n",
    "from custom_tranformations import RandomRotation\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from utils import encode_onehot, custom_label_binarize\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('CUDA is not available. Using CPU.')\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                             Global Configurations                             #\n",
    "################################################################################\n",
    "\n",
    "KFOLD_PATH = r\"E:\\Aamir Gulzar\\dataset\\splits\\kfolds_IDARS.csv\"\n",
    "DATA_PATH = r\"E:\\Aamir Gulzar\\dataset\\patches\"\n",
    "\n",
    "# GPU memory limit (commented out in your code)\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "\n",
    "# Some global metrics placeholders\n",
    "lmbda = 0.1\n",
    "best_auc_v = 0\n",
    "best_auc = 0\n",
    "n_slides = 0\n",
    "best_loss = 100000.\n",
    "best_f1_v = 0.\n",
    "best_Acc = 0.\n",
    "best_ap_v = 0.\n",
    "\n",
    "num_classes = 2\n",
    "label_names = ['nonMSI', 'MSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--budget'], dest='budget', nargs=None, const=None, default=0.8, type=<class 'float'>, choices=None, required=False, help='the budget for how often the network can get hints', metavar='N')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='MSI And MSS Classification')\n",
    "parser.add_argument('--data_lib', type=str, default=KFOLD_PATH, help='path to train ')\n",
    "parser.add_argument('--val_lib', type=str, default=KFOLD_PATH, help='path to validation ')\n",
    "parser.add_argument('--test_lib', type=str, default=KFOLD_PATH, help='path to validation ')\n",
    "parser.add_argument('--problem', type=str, default='MSI_vs_MSS', help='classification problem.')\n",
    "parser.add_argument('--pos_label', type=int, default=1, help='positive label.')\n",
    "parser.add_argument('--neg_label', type=int, default=0, help='negative label. If present.')\n",
    "parser.add_argument('--output', type=str, default='Baseline_Fivecrop_4Folds', help='now i am using t=1 and r=10')\n",
    "parser.add_argument('--folds', type=int, default=4, help='number of fold to execute')\n",
    "parser.add_argument('--batch_size', type=int, default=512, help='mini-batch size (default: 128)')\n",
    "parser.add_argument('--nepochs', type=int, default=4, help='number of epochs')\n",
    "parser.add_argument('--workers', default=0, type=int, help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--test_every', default=1, type=int, help='test on val every (default: 10)')\n",
    "parser.add_argument('--weights', default=0.5, type=float, help='unbalanced positive class weight (default: 0.5, balanced classes)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.001) # best 0.01\n",
    "parser.add_argument('--l2_reg', type=float, default=5e-3)\n",
    "parser.add_argument('--grad_bound', type=float, default=5.0)\n",
    "\n",
    "parser.add_argument('--r', default=10, type=int, help='how many rand tiles to consider (default: 10)')\n",
    "parser.add_argument('--k', default=1, type=int, help='how many top k tiles to consider (default: 10)')\n",
    "\n",
    "parser.add_argument('--budget', type=float, default=0.8, metavar='N',\n",
    "                    help='the budget for how often the network can get hints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                     Aggregation Class (for tile-level merges)                #\n",
    "################################################################################\n",
    "\n",
    "class Aggregation:\n",
    "    \"\"\"\n",
    "    This class holds all tile-level aggregation methods. \n",
    "    It can be configured via 'aggregation_config' to enable or disable certain\n",
    "    aggregations or to conditionally save results to CSV, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def group_max(groups, data, nmax):\n",
    "        \"\"\"\n",
    "        Return the maximum value in each group. \n",
    "        nmax is the maximum group ID + 1, used to fill results in an array.\n",
    "        \"\"\"\n",
    "        out = np.empty(nmax)\n",
    "        out[:] = np.nan\n",
    "        order = np.lexsort((data, groups))\n",
    "        groups = groups[order]\n",
    "        data = data[order]\n",
    "        index = np.empty(len(groups), 'bool')\n",
    "        index[-1] = True\n",
    "        index[:-1] = groups[1:] != groups[:-1]\n",
    "        out[groups[index]] = data[index]\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def group_avg(groups, data):\n",
    "        \"\"\"\n",
    "        Compute mean of data for each unique group.\n",
    "        \"\"\"\n",
    "        order = np.lexsort((data, groups))\n",
    "        groups = groups[order]\n",
    "        data = data[order]\n",
    "        unames, idx, counts = np.unique(groups, return_inverse=True, return_counts=True)\n",
    "        sum_pred = np.bincount(idx, weights=data)\n",
    "        mean_pred = sum_pred / counts\n",
    "        return mean_pred, sum_pred\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_aggregated_probabilities(group, data):\n",
    "        \"\"\"\n",
    "        **Reduced** aggregator that returns only avg & max\n",
    "        for each group. \n",
    "        \"\"\"\n",
    "        wsi_dict = {}\n",
    "        for idx, g in enumerate(group):\n",
    "            if g not in wsi_dict:\n",
    "                wsi_dict[g] = [data[idx]]\n",
    "            else:\n",
    "                wsi_dict[g].append(data[idx])\n",
    "\n",
    "        avg_p = []\n",
    "        max_p = []\n",
    "        # (Removed sum, median, gmean, top-half-mean)\n",
    "\n",
    "        for each_wsi in wsi_dict.keys():\n",
    "            wsi_predictions = np.array(wsi_dict[each_wsi], dtype='float64')\n",
    "            avg_p.append(np.mean(wsi_predictions))  # keep only average\n",
    "            max_p.append(np.max(wsi_predictions))   # keep only max\n",
    "\n",
    "        avg_p = np.array(avg_p, dtype='float64')\n",
    "        max_p = np.array(max_p, dtype='float64')\n",
    "        return avg_p, max_p\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_aggregated_predictions(group, data):\n",
    "        \"\"\"\n",
    "        Compute majority-vote predictions for each WSI. \n",
    "        \"\"\"\n",
    "        wsi_dict = {}\n",
    "        for idx, g in enumerate(group):\n",
    "            if g not in wsi_dict:\n",
    "                wsi_dict[g] = [data[idx]]\n",
    "            else:\n",
    "                wsi_dict[g].append(data[idx])\n",
    "\n",
    "        mv_pred = []\n",
    "        n_pred = []\n",
    "        for each_wsi in wsi_dict.keys():\n",
    "            wsi_predictions = wsi_dict[each_wsi]\n",
    "            wsi_pred_class = []\n",
    "            for cl in range(num_classes):\n",
    "                wsi_pred_class.append(wsi_predictions.count(cl))\n",
    "\n",
    "            mj_vt = np.argmax(wsi_pred_class)\n",
    "            mv_pred.append(mj_vt)\n",
    "            n_pred.append(wsi_pred_class)\n",
    "\n",
    "        n_pred = np.array(n_pred, dtype='float64')\n",
    "        mv_pred = np.array(mv_pred, dtype='float64')\n",
    "        return mv_pred, n_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def group_avg_df(groups, data):\n",
    "        \"\"\"\n",
    "        Group top-10 aggregator using pandas. \n",
    "        Returns the mean of nlargest(10) for each group.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'Slide': groups, 'value': data})\n",
    "        group_average_df = df.groupby('Slide')['value'].apply(lambda grp: grp.nlargest(10).mean())\n",
    "        group_average = group_average_df.tolist()\n",
    "        return group_average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                     Evaluation Class (metrics, plotting, etc.)               #\n",
    "################################################################################\n",
    "\n",
    "class Evaluation:\n",
    "    \"\"\"\n",
    "    This class holds evaluation-related methods such as metrics calculation,\n",
    "    confusion matrices, classification reports, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "    @staticmethod\n",
    "    def cutoff_youdens_j(fpr, tpr, thresholds):\n",
    "        j_scores = tpr - fpr\n",
    "        j_ordered = sorted(zip(j_scores, thresholds))\n",
    "        return j_ordered[-1][1]\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_f1_score(targets, prediction, cutoff):\n",
    "        prediction = np.array(prediction)\n",
    "        targets = np.array(targets)\n",
    "        return f1_score(targets, prediction, average='weighted')\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_metrics(target, prediction):\n",
    "        fpr, tpr, thresholds = roc_curve(target, prediction)\n",
    "        cutoff = Evaluation.cutoff_youdens_j(fpr, tpr, thresholds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        f1score = Evaluation.cal_f1_score(target, prediction, cutoff)\n",
    "        precision, recall, _ = precision_recall_curve(target, prediction, zero_division=1)\n",
    "        average_precision = average_precision_score(target, prediction, zero_division=1)\n",
    "        return f1score, average_precision, roc_auc, cutoff\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_accuracy(output, target):\n",
    "        preds = output.max(1, keepdim=True)[1]\n",
    "        correct = preds.eq(target.view_as(preds)).sum()\n",
    "        acc = correct.float() / preds.shape[0]\n",
    "        return acc\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_auc(labels, predictions):\n",
    "        # binary_labels = custom_label_binarize(np.array(labels), classes=[i for i in range(num_classes)])\n",
    "        # auc_list = []\n",
    "        # for cl in range(num_classes):\n",
    "        #     fpr, tpr, thresholds = roc_curve(binary_labels[:, cl], predictions[:, cl])\n",
    "        #     auc_list.append(auc(fpr, tpr))\n",
    "        aucscore = roc_auc_score(labels, predictions[:, 1])\n",
    "        return aucscore\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_metrics(target, prediction, set):\n",
    "        import matplotlib.pyplot as plt\n",
    "        fpr, tpr, thresholds = roc_curve(target, prediction)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print('roc_auc is:', roc_auc)\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(target, prediction, zero_division=1)\n",
    "        average_precision = average_precision_score(target, prediction, zero_division=1)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        lw = 2\n",
    "\n",
    "        # Subplot 1: ROC\n",
    "        plt.subplot(121)\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve, AUC={0:0.2f}'.format(roc_auc))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        # Subplot 2: Precision-Recall\n",
    "        plt.subplot(122)\n",
    "        plt.step(recall, precision, alpha=0.4, color='darkorange', where='post')\n",
    "        plt.fill_between(recall, precision, alpha=0.2, color='navy', step='post')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0, 1.05])\n",
    "        plt.xlim([0, 1])\n",
    "        plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "\n",
    "        plt.savefig(os.path.join(args.output, 'roc_pr' + set + '.png'))\n",
    "        plt.close(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                            Training/Testing Utilities                        #\n",
    "################################################################################\n",
    "\n",
    "def inference(run, loader, model, criterion):\n",
    "    model.eval()\n",
    "    running_acc = 0.\n",
    "    running_loss = 0.\n",
    "    probs = torch.FloatTensor(len(loader.dataset), num_classes)\n",
    "    preds = torch.FloatTensor(len(loader.dataset))\n",
    "\n",
    "    index_offset = 0  # --- 5-crop change ---\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, target) in enumerate(loader):\n",
    "            try:\n",
    "                # inputs shape: (batch_size, 5, 3, 224, 224) for 5-crop\n",
    "                b, ncrops, c, h, w = inputs.shape\n",
    "                inputs = inputs.view(-1, c, h, w).cuda()     # shape: (batch_size*5, 3, 224, 224)\n",
    "                target = target.cuda()\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(inputs)                       # shape: (batch_size*5, num_classes)\n",
    "\n",
    "                # Average logits over the 5 crops to get a single prediction per sample\n",
    "                output = output.view(b, ncrops, -1).mean(1)  # shape: (batch_size, num_classes)\n",
    "                loss = criterion(output, target)\n",
    "                acc = Evaluation.calculate_accuracy(output, target)\n",
    "                y = F.softmax(output, dim=-1)\n",
    "                pred_value, pred = torch.max(output.data, 1)\n",
    "\n",
    "                # Fill in our large preds/probs arrays\n",
    "                batch_size_now = b\n",
    "                preds[index_offset:index_offset + batch_size_now] = pred.detach().clone()\n",
    "                probs[index_offset:index_offset + batch_size_now] = y.detach().clone()\n",
    "                index_offset += batch_size_now\n",
    "\n",
    "                running_acc += acc.item() * batch_size_now\n",
    "                running_loss += loss.item() * batch_size_now\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print('Inference\\tEpoch: [{:3d}/{:3d}]\\tBatch: [{:3d}/{}]\\tValidation Loss: {:.4f}, acc: {:0.2f}%'\n",
    "                        .format(run + 1, args.nepochs, i + 1, len(loader),\n",
    "                                running_loss / ((i + 1) * batch_size_now),\n",
    "                                (100 * running_acc) / ((i + 1) * batch_size_now)))\n",
    "            except RuntimeError as e:\n",
    "                print(f\"RuntimeError in batch {i}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue  # Skip to the next batch\n",
    "    return (\n",
    "        probs.cpu().numpy(),\n",
    "        running_loss / len(loader.dataset),\n",
    "        running_acc / len(loader.dataset),\n",
    "        preds.cpu().numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "def train(run, loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    index_offset = 0  # --- 5-crop change ---\n",
    "    for i, (inputs, target) in enumerate(loader):\n",
    "        try:\n",
    "            # inputs shape: (batch_size, 5, 3, 224, 224) for 5-crop\n",
    "            # print('5. i am running upto here inside train \\n\\n')\n",
    "            b, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w).cuda()  # shape: (batch_size*5, 3, 224, 224)\n",
    "            target = target.cuda()\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)                    # shape: (batch_size*5, num_classes)\n",
    "            # print('6. i am running upto here model given me output \\n\\n')\n",
    "            # Average over 5 crops\n",
    "            output = output.view(b, ncrops, -1).mean(1)  # shape: (batch_size, num_classes)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * b\n",
    "            acc = Evaluation.calculate_accuracy(output, target)\n",
    "            running_acc += acc.item() * b\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(\"Train Epoch: [{:3d}/{:3d}] Batch: {:3d}, Training Loss: {:.4f}, Acc: {:.2f}%\"\n",
    "                    .format(run + 1, args.nepochs, i + 1,\n",
    "                            running_loss / ((i + 1) * b),\n",
    "                            100 * running_acc / ((i + 1) * b)))\n",
    "        except RuntimeError as e:\n",
    "                    print(f\"RuntimeError in batch {i}: {e}\")\n",
    "                    torch.cuda.empty_cache()\n",
    "                    continue  # Skip to the next batch\n",
    "    return (running_loss / len(loader.dataset),\n",
    "            running_acc / len(loader.dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                                Dataset Class                                 #\n",
    "################################################################################\n",
    "def get_split_indices(lib, test_fold, val_fold):\n",
    "    \"\"\"\n",
    "    lib: DataFrame with a column named 'fold' \n",
    "         that has values in [0, 1, 2, 3].\n",
    "    test_fold, val_fold: integers from 0..3\n",
    "    returns: three lists => train_idx, val_idx, test_idx\n",
    "    \"\"\"\n",
    "    # Extract all folds\n",
    "    all_folds = lib['fold'].values.tolist()\n",
    "    test_idx = [i for i, f in enumerate(all_folds) if f == test_fold]\n",
    "    val_idx  = [i for i, f in enumerate(all_folds) if f == val_fold]\n",
    "    train_idx = [i for i, f in enumerate(all_folds)\n",
    "                 if f not in (test_fold, val_fold)]\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "class MILdataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset to handle multiple WSI tiles from a library file (CSV).\n",
    "    Each tile is read at 512×512 and then transformed into 5-crop (224×224).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, libraryfile=KFOLD_PATH, transform=None, mult=2, s=10, \n",
    "                 shuffle=False, set='', test_fold=None, val_fold=None):\n",
    "        path_dir = DATA_PATH\n",
    "        lib = pd.DataFrame(pd.read_csv(libraryfile, usecols=[\n",
    "            'Case_ID', 'WSI_Dir', 'label_desc', 'label_id', f'fold'\n",
    "        ]))\n",
    "\n",
    "        lib.dropna(inplace=True)\n",
    "\n",
    "        allcases = lib['Case_ID'].values.tolist()\n",
    "        allslides = lib['WSI_Dir'].values.tolist()\n",
    "        tar = lib['label_id'].values.tolist()\n",
    "        label_desc = lib['label_desc'].values.tolist()\n",
    "        split = lib[f'fold'].values.tolist()\n",
    "\n",
    "        train_idx, val_idx, test_idx = get_split_indices(lib, test_fold, val_fold)\n",
    "        if set == 'train':\n",
    "            print(f\"##--Targets ==> {len(tar)} | {tar.count(0)} | {tar.count(1)} --##\")\n",
    "            indices = train_idx\n",
    "            print(f\"##--Train Split ==> {len(indices)} Slides\")\n",
    "            thresh_tiles=3\n",
    "        elif set == 'valid':\n",
    "            indices = val_idx\n",
    "            print(f\"##--Val Split ==> {len(indices)} Slides\")\n",
    "            thresh_tiles=3\n",
    "        elif set == 'test':\n",
    "            indices = test_idx\n",
    "            print(f\"##--Test Split ==> {len(indices)} Slides\")\n",
    "            thresh_tiles=3\n",
    "        else:\n",
    "            raise ValueError(\"Invalid set_type. Must be 'train', 'valid', or 'test'.\")\n",
    "\n",
    "        cases = []\n",
    "        tiles = []\n",
    "        ntiles = []\n",
    "        slideIDX = []\n",
    "        targetIDX = []\n",
    "        targets = []\n",
    "        label_desciption = []\n",
    "        slides = []\n",
    "\n",
    "        j = 0\n",
    "        for i in indices:\n",
    "            path = os.path.join(path_dir, str(allslides[i]))\n",
    "            slide_label = int(tar[i])\n",
    "            if os.path.exists(path):\n",
    "                # max patch per slide comment it if you want to use all patches or make MAX_PATCHES = len(t)\n",
    "                # MAX_PATCHES = 10\n",
    "                t = [\n",
    "                    os.path.join(path, f) \n",
    "                    for f in os.listdir(path) \n",
    "                    if f.endswith('.png')\n",
    "                ]\n",
    "                if len(t) >= thresh_tiles:\n",
    "                    cases.append(allcases[i])\n",
    "                    slides.append(allslides[i])\n",
    "                    tiles.extend(t)\n",
    "                    ntiles.append(len(t))\n",
    "                    slideIDX.extend([j]*len(t))\n",
    "                    targetIDX.extend([slide_label]*len(t))\n",
    "                    targets.append(slide_label)\n",
    "                    label_desciption.append(label_desc[i])\n",
    "                    j += 1\n",
    "\n",
    "        print('-------------------------')\n",
    "        print('Number of Slides: {}'.format(len(slides)))\n",
    "        print('Number of tiles: {}'.format(len(tiles)))\n",
    "        print('Max tiles: ', max(ntiles) if len(ntiles) else 0)\n",
    "        print('Min tiles: ', min(ntiles) if len(ntiles) else 0)\n",
    "        print('Average tiles: ', np.mean(ntiles) if len(ntiles) else 0)\n",
    "        print('nonMSI: ', targets.count(0))\n",
    "        print('MSI: ', targets.count(1))\n",
    "\n",
    "        self.slideIDX = slideIDX\n",
    "        self.ntiles = ntiles\n",
    "        self.tiles = tiles\n",
    "        self.targets = targets\n",
    "        self.label_desc = label_desciption\n",
    "        self.slides = slides\n",
    "        self.cases = cases\n",
    "\n",
    "        self.transform = transform\n",
    "        self.mult = mult\n",
    "        self.s = s\n",
    "        self.mode = None\n",
    "        self.shuffle = shuffle\n",
    "        self.targetIDX = targetIDX\n",
    "\n",
    "    def setmode(self, mode):\n",
    "        self.mode = mode\n",
    "\n",
    "    def maketraindata(self, idxs):\n",
    "        self.t_data = [(self.slideIDX[x], self.tiles[x], self.targets[self.slideIDX[x]]) \n",
    "                       for x in idxs]\n",
    "\n",
    "    def shuffletraindata(self):\n",
    "        self.t_data = random.sample(self.t_data, len(self.t_data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 1:\n",
    "            tile = self.tiles[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "\n",
    "            # --- 5-crop change ---\n",
    "            # Remove manual resize; rely on self.transform pipeline\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)  # => shape: (5, 3, 224, 224)\n",
    "            # 'img' is now a 5-crop tensor\n",
    "\n",
    "            slide_idx = self.slideIDX[index]\n",
    "            target = self.targets[slide_idx]\n",
    "            return img, target\n",
    "\n",
    "        elif self.mode == 2:\n",
    "            slideIDX, tile, target = self.t_data[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "\n",
    "            # --- 5-crop change ---\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)  # => shape: (5, 3, 224, 224)\n",
    "\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 1:\n",
    "            return len(self.tiles)\n",
    "        elif self.mode == 2:\n",
    "            return len(self.t_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--Targets ==> 405 | 344 | 61 --##\n",
      "##--Train Split ==> 205 Slides\n",
      "-------------------------\n",
      "Number of Slides: 205\n",
      "Number of tiles: 123284\n",
      "Max tiles:  1849\n",
      "Min tiles:  6\n",
      "Average tiles:  601.3853658536585\n",
      "nonMSI:  172\n",
      "MSI:  33\n",
      "##--Val Split ==> 100 Slides\n",
      "-------------------------\n",
      "Number of Slides: 100\n",
      "Number of tiles: 55849\n",
      "Max tiles:  1942\n",
      "Min tiles:  30\n",
      "Average tiles:  558.49\n",
      "nonMSI:  85\n",
      "MSI:  15\n",
      "##--Test Split ==> 100 Slides\n",
      "-------------------------\n",
      "Number of Slides: 100\n",
      "Number of tiles: 64050\n",
      "Max tiles:  1875\n",
      "Min tiles:  18\n",
      "Average tiles:  640.5\n",
      "nonMSI:  87\n",
      "MSI:  13\n",
      "205\n",
      "Train Epoch: [  1/  4] Batch:   1, Training Loss: 0.7160, Acc: 48.63%\n",
      "Train Epoch: [  1/  4] Batch: 101, Training Loss: 0.3127, Acc: 87.36%\n",
      "Train Epoch: [  1/  4] Batch: 201, Training Loss: 0.2722, Acc: 89.01%\n",
      "Training\tEpoch: [1/4]\tLoss: 0.2610\tAccuracy:  89\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/110]\tValidation Loss: 0.6878, acc: 77.15%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [101/110]\tValidation Loss: 0.6829, acc: 77.91%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.582  AVG=0.634  MAX=0.745  top10=0.654\n",
      "Balanced Acc (Val): MV=0.567  AVG=0.600  MAX=0.694  top10=0.622\n",
      "AUC-scores (Val): AVG=0.703 MAX=0.824 top10=0.719\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Saved checkpoint_best_AUC.pth\n",
      "Train Epoch: [  2/  4] Batch:   1, Training Loss: 0.1664, Acc: 93.36%\n",
      "Train Epoch: [  2/  4] Batch: 101, Training Loss: 0.1829, Acc: 92.60%\n",
      "Train Epoch: [  2/  4] Batch: 201, Training Loss: 0.1750, Acc: 92.98%\n",
      "Training\tEpoch: [2/4]\tLoss: 0.1725\tAccuracy:  93\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/110]\tValidation Loss: 0.7390, acc: 78.71%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [101/110]\tValidation Loss: 0.8270, acc: 80.80%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.524  AVG=0.524  MAX=0.654  top10=0.722\n",
      "Balanced Acc (Val): MV=0.533  AVG=0.533  MAX=0.622  top10=0.667\n",
      "AUC-scores (Val): AVG=0.766 MAX=0.781 top10=0.779\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Train Epoch: [  3/  4] Batch:   1, Training Loss: 0.1538, Acc: 94.73%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################\n",
    "#                                Main Pipeline                                 #\n",
    "################################################################################\n",
    "class StackAndNormalize:\n",
    "    def __init__(self, normalize):\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, crops):\n",
    "        return torch.stack([\n",
    "            self.normalize(transforms.ToTensor()(crop)) \n",
    "            for crop in crops\n",
    "        ])\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training/validation/test pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    global args, best_auc_v, tr_batch_size, n_slides, best_auc, best_f1_v\n",
    "\n",
    "    config = {\n",
    "        'aggregation_methods': ['average', 'top10', 'max'],\n",
    "        'save_val_tile_csv': True,\n",
    "        'save_test_tile_csv': True,\n",
    "        'save_val_slide_csv': True,\n",
    "        'save_test_slide_csv': True,\n",
    "        'enable_group_avg_df': True,\n",
    "        'enable_plot_metrics': True,\n",
    "        'enable_confusion_matrix': True\n",
    "    }\n",
    "\n",
    "    aggregator = Aggregation(config=config)\n",
    "    evaluator = Evaluation(config=config)\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    if not os.path.exists(args.output):\n",
    "        os.mkdir(args.output)\n",
    "\n",
    "    args.output = os.path.join(args.output, args.problem)\n",
    "    if not os.path.exists(args.output):\n",
    "        os.mkdir(args.output)\n",
    "\n",
    "    temp_output = args.output\n",
    "\n",
    "    AUC_SCORES = []\n",
    "    F1_SCORES = []\n",
    "    # Add these variables to control resume functionality\n",
    "    resume_fold = 4  # Specify the fold to resume from (1-indexed)\n",
    "    resume_epoch = 2  # Specify the epoch to resume from (0-indexed)\n",
    "    for fold in range(args.folds):\n",
    "        if fold + 1 < resume_fold:  # Skip folds before the resume point\n",
    "            continue\n",
    "        test_fold = fold + 1\n",
    "        val_fold  = ((fold + 1) % 4) + 1\n",
    "        args.output = os.path.join(temp_output, 'fold' + str(fold + 1))\n",
    "        if not os.path.exists(args.output):\n",
    "            os.mkdir(args.output)\n",
    "        path_fold = args.output\n",
    "\n",
    "        for sets in range(1):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            args.output = os.path.join(path_fold, 'best' + str(sets))\n",
    "            if not os.path.exists(args.output):\n",
    "                os.mkdir(args.output)\n",
    "\n",
    "            global best_auc_v, best_auc, n_slides, best_loss, best_f1_v, best_Acc, best_ap_v\n",
    "            best_auc_v = 0\n",
    "            best_auc = 0\n",
    "            n_slides = 0\n",
    "            best_loss = 100000.\n",
    "            best_f1_v = 0.\n",
    "            best_Acc = 0.\n",
    "            best_ap_v = 0.\n",
    "\n",
    "            model = models.resnet34(weights='DEFAULT')\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "            model.cuda()\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "            cudnn.benchmark = True\n",
    "            criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "               # --- 5-crop change: define transforms so we return 5-crops (224×224) for each 512×512 image\n",
    "            normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.1, 0.1, 0.1])\n",
    "            trans = transforms.Compose([\n",
    "                RandomRotation([0, 90, 180, 270]),\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.FiveCrop(224),   # returns tuple of 5 PIL images\n",
    "                StackAndNormalize(normalize)])\n",
    "\n",
    "            trans_Valid = transforms.Compose([\n",
    "                transforms.FiveCrop(224),\n",
    "                StackAndNormalize(normalize)])\n",
    "            # --- end 5-crop changes\n",
    "\n",
    "            # Load Data\n",
    "            train_dset = MILdataset(args.data_lib, trans, set='train',  test_fold=test_fold, val_fold=val_fold)\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_dset,\n",
    "                batch_size=args.batch_size, shuffle=False,\n",
    "                num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "            val_dset = MILdataset(args.data_lib, trans_Valid, set='valid',  test_fold=test_fold, val_fold=val_fold)\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                val_dset,\n",
    "                batch_size=args.batch_size, shuffle=False,\n",
    "                num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "            test_dset = MILdataset(args.data_lib, trans_Valid, set='test',  test_fold=test_fold, val_fold=val_fold)\n",
    "            test_loader = torch.utils.data.DataLoader(\n",
    "                test_dset,\n",
    "                batch_size=args.batch_size, shuffle=False,\n",
    "                num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "            #  if you wanted to see only dataset distribution and data loaders etc then put continue here\n",
    "            # continue\n",
    "            fconv = open(os.path.join(args.output, 'train_convergence.csv'), 'w')\n",
    "            fconv.write('epoch,loss,accuracy\\n')\n",
    "            fconv.close()\n",
    "\n",
    "            fconv = open(os.path.join(args.output, 'valid_convergence.csv'), 'w')\n",
    "            # We removed sum, median, gmean columns; only keep relevant columns\n",
    "            fconv.write('epoch,tile_loss,tile_acc,best_F1,F1_AVG,F1_Max,F1_T10,best_BAcc,Bacc_AVG,Bacc_Max,Bacc_T10,best_AUC,Avg_AUC,Max_AUC,Top_AUC\\n')\n",
    "            fconv.close()\n",
    "\n",
    "            num_tiles = len(train_dset.slideIDX)\n",
    "            n_slides = len(train_dset.slides)\n",
    "            print(n_slides)\n",
    "            train_dset.maketraindata(np.arange(num_tiles))\n",
    "            # Resume logic\n",
    "            # start_epoch = 0\n",
    "            # if fold + 1 == resume_fold:\n",
    "            #     checkpoint_path = r\"E:\\Aamir Gulzar\\existing_approaches\\Baseline_Fivecrop_4Folds\\MSI_vs_MSS\\fold3\\best0\\checkpoint_best_AUC.pth\"\n",
    "            #     if os.path.exists(checkpoint_path):\n",
    "            #         checkpoint = torch.load(checkpoint_path)\n",
    "            #         model.load_state_dict(checkpoint['state_dict'])\n",
    "            #         optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            #         start_epoch = resume_epoch\n",
    "            #         print(f\"Resuming training from Fold {resume_fold}, Epoch {resume_epoch + 1}\")\n",
    "\n",
    "            for epoch in range(args.nepochs):\n",
    "                train_dset.shuffletraindata()\n",
    "                train_dset.setmode(2)\n",
    "                loss, acc = train(epoch, train_loader, model, criterion, optimizer)\n",
    "                print('Training\\tEpoch: [{}/{}]\\tLoss: {:0.4f}\\tAccuracy: {:3d}'\n",
    "                      .format(epoch+1, args.nepochs, loss, int(acc * 100)))\n",
    "\n",
    "                fconv = open(os.path.join(args.output, 'train_convergence.csv'), 'a')\n",
    "                fconv.write('{},{:0.4f},{:3d}\\n'.format(epoch, loss, int(acc * 100)))\n",
    "                fconv.close()\n",
    "\n",
    "                # Validation\n",
    "                if (epoch + 1) % args.test_every == 0:\n",
    "                    val_dset.setmode(1)\n",
    "                    val_probs, val_loss, val_acc, val_preds = inference(epoch, val_loader, model, criterion)\n",
    "                    val_slide_mjvt, _ = aggregator.compute_aggregated_predictions(np.array(val_dset.slideIDX), val_preds)\n",
    "\n",
    "                    # We only keep avg, max, top10\n",
    "                    val_slide_avg = []\n",
    "                    val_slide_max = []\n",
    "                    val_slide_avgt10 = []\n",
    "\n",
    "                    # We'll store each class aggregator\n",
    "                    for cl in range(num_classes):\n",
    "                        t_avg, t_max = aggregator.compute_aggregated_probabilities(\n",
    "                            np.array(val_dset.slideIDX), val_probs[:, cl]\n",
    "                        )\n",
    "                        t_t10 = aggregator.group_avg_df(\n",
    "                            np.array(val_dset.slideIDX), val_probs[:, cl]\n",
    "                        )\n",
    "                        val_slide_avg.append(t_avg)\n",
    "                        val_slide_max.append(t_max)\n",
    "                        val_slide_avgt10.append(t_t10)\n",
    "\n",
    "                    val_slide_avg = np.array(val_slide_avg).transpose()\n",
    "                    val_slide_max = np.array(val_slide_max).transpose()\n",
    "                    val_slide_avgt10 = np.array(val_slide_avgt10).transpose()\n",
    "\n",
    "                    val_slide_avg_m = np.argmax(val_slide_avg, axis=1)\n",
    "                    val_slide_max_m = np.argmax(val_slide_max, axis=1)\n",
    "                    val_slide_avgt10_m = np.argmax(val_slide_avgt10, axis=1)\n",
    "\n",
    "                    from sklearn.metrics import f1_score\n",
    "                    f1_mv = f1_score(val_dset.targets, val_slide_mjvt, average='macro')\n",
    "                    f1_avg = f1_score(val_dset.targets, val_slide_avg_m, average='macro')\n",
    "                    f1_max = f1_score(val_dset.targets, val_slide_max_m, average='macro')\n",
    "                    f1_a10 = f1_score(val_dset.targets, val_slide_avgt10_m, average='macro')\n",
    "                    bacc_mv = balanced_accuracy_score(val_dset.targets, val_slide_mjvt)\n",
    "                    bacc_avg = balanced_accuracy_score(val_dset.targets, val_slide_avg_m)\n",
    "                    bacc_max = balanced_accuracy_score(val_dset.targets, val_slide_max_m)\n",
    "                    bacc_avgt10 = balanced_accuracy_score(val_dset.targets, val_slide_avgt10_m)\n",
    "\n",
    "                    auc_val_avg = evaluator.compute_auc(val_dset.targets, val_slide_avg)\n",
    "                    auc_val_max = evaluator.compute_auc(val_dset.targets, val_slide_max)\n",
    "                    auc_val_top10 = evaluator.compute_auc(val_dset.targets, val_slide_avgt10)\n",
    "\n",
    "                    fconv = open(os.path.join(args.output, 'valid_convergence.csv'), 'a')\n",
    "                    fconv.write('{},{:0.4f},{:3d},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f}\\n'.format(\n",
    "                        epoch, val_loss, int(val_acc * 100),\n",
    "                        max(f1_mv,f1_avg, f1_max, f1_a10),\n",
    "                        f1_avg, f1_max, f1_a10,\n",
    "                        # save max and individual balanced accuracies\n",
    "                        max(bacc_mv, bacc_avg, bacc_max, bacc_avgt10),\n",
    "                        bacc_avg, bacc_max, bacc_avgt10,\n",
    "                        # also save best auc values\n",
    "                        max(auc_val_avg, auc_val_max, auc_val_top10),\n",
    "                        auc_val_avg, auc_val_max, auc_val_top10\n",
    "                    ))\n",
    "                    fconv.close()\n",
    "\n",
    "                    print(\"\\n----------------------Validation Results -------------------------------------\")\n",
    "                    print(f\"F1-scores (Val): MV={f1_mv:.3f}  AVG={f1_avg:.3f}  MAX={f1_max:.3f}  top10={f1_a10:.3f}\")\n",
    "                    print(f\"Balanced Acc (Val): MV={bacc_mv:.3f}  AVG={bacc_avg:.3f}  MAX={bacc_max:.3f}  top10={bacc_avgt10:.3f}\")\n",
    "                    print(f\"AUC-scores (Val): AVG={auc_val_avg:.3f} MAX={auc_val_max:.3f} top10={auc_val_top10:.3f}\")\n",
    "                    print(\"------------------------------------------------------------------\\n\")\n",
    "\n",
    "                    best_f1_candidate = max(f1_mv, f1_avg, f1_max, f1_a10)\n",
    "                    if best_f1_candidate > best_f1_v:\n",
    "                        best_f1_v = best_f1_candidate\n",
    "                        obj = {\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_ap_v': best_f1_v,\n",
    "                            'best_auc_v': max(auc_val_max, auc_val_top10, auc_val_avg),\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "                        torch.save(obj, os.path.join(args.output, 'checkpoint_best_F1.pth'))\n",
    "                        print(\"Saved checkpoint_best_F1.pth\")\n",
    "                        # save the tile level predictions for validation slides columns slideidx, prob non msi, prob msi and pred tile\n",
    "                        df2_f1 = pd.DataFrame({\n",
    "                            'slideidx': val_dset.slideIDX,\n",
    "                            'nonMSI_prob': val_probs[:, 0],\n",
    "                            'MSI_prob': val_probs[:, 1],\n",
    "                            'pred_tile': val_preds\n",
    "                        })\n",
    "                        df2_f1.to_csv(os.path.join(args.output, 'val_tile_pred_F1.csv'), index=False)\n",
    "\n",
    "                    best_auc_candidate = max(auc_val_avg, auc_val_max, auc_val_top10)\n",
    "                    if best_auc_candidate > best_auc_v:\n",
    "                        best_auc_v = best_auc_candidate\n",
    "                        obj = {\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_ap_v': best_f1_v,\n",
    "                            'best_auc_v': best_auc_v,\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "                        torch.save(obj, os.path.join(args.output, 'checkpoint_best_AUC.pth'))\n",
    "                        print(\"Saved checkpoint_best_AUC.pth\")\n",
    "\n",
    "                        # save the tile level predictions for validation slides columns slideidx, prob non msi, prob msi and pred tile\n",
    "                        df2_auc = pd.DataFrame({\n",
    "                            'slideidx': val_dset.slideIDX,\n",
    "                            'nonMSI_prob': val_probs[:, 0],\n",
    "                            'MSI_prob': val_probs[:, 1],\n",
    "                            'pred_tile': val_preds\n",
    "                        })\n",
    "                        df2_auc.to_csv(os.path.join(args.output, 'val_tile_pred_AUC.csv'), index=False)\n",
    "\n",
    "            # Save ground truth for validation slides\n",
    "            df1 = pd.DataFrame({\n",
    "                'Case_ID': val_dset.cases,\n",
    "                'WSI_Id': val_dset.slides,\n",
    "                'n_tiles': val_dset.ntiles,\n",
    "                'label_desc': val_dset.label_desc,\n",
    "                'label_id': val_dset.targets\n",
    "            })\n",
    "            df1.to_csv(os.path.join(args.output, 'val_GT.csv'), index=False)\n",
    "            df2_f1.to_csv(os.path.join(args.output, 'val_tile_pred_F1.csv'), index=False)\n",
    "            df2_auc.to_csv(os.path.join(args.output, 'val_tile_pred_AUC.csv'), index=False)\n",
    "        \n",
    "            ############## Test ##############\n",
    "            ch = torch.load(os.path.join(args.output, 'checkpoint_best_AUC.pth'))\n",
    "            model.load_state_dict(ch['state_dict'])\n",
    "\n",
    "            test_dset.setmode(1)\n",
    "            test_probs, test_loss, test_acc, test_preds = inference(epoch, test_loader, model, criterion)\n",
    "\n",
    "            print(f'Predicted Tiles: {len(test_probs[:, 1])}, classes: {len(test_probs[0, :])}, total targets: {len(test_dset.targets)}')\n",
    "\n",
    "            test_slide_mjvt, _ = aggregator.compute_aggregated_predictions(\n",
    "                np.array(test_dset.slideIDX), test_preds\n",
    "            )\n",
    "            test_slide_avg = []\n",
    "            test_slide_max = []\n",
    "            test_slide_avgt10 = []\n",
    "            for cl in range(num_classes):\n",
    "                t_avg, t_max = aggregator.compute_aggregated_probabilities(\n",
    "                    np.array(test_dset.slideIDX), test_probs[:, cl]\n",
    "                )\n",
    "                t_t10 = aggregator.group_avg_df(\n",
    "                    np.array(test_dset.slideIDX), test_probs[:, cl]\n",
    "                )\n",
    "                test_slide_avg.append(t_avg)\n",
    "                test_slide_max.append(t_max)\n",
    "                test_slide_avgt10.append(t_t10)\n",
    "\n",
    "            test_slide_avg = np.array(test_slide_avg).transpose()\n",
    "            test_slide_max = np.array(test_slide_max).transpose()\n",
    "            test_slide_avgt10 = np.array(test_slide_avgt10).transpose()\n",
    "\n",
    "            test_slide_avg_m = np.argmax(test_slide_avg, axis=1)\n",
    "            test_slide_max_m = np.argmax(test_slide_max, axis=1)\n",
    "            test_slide_avgt10_m = np.argmax(test_slide_avgt10, axis=1)\n",
    "\n",
    "            f1_mv = f1_score(test_dset.targets, test_slide_mjvt, average='macro')\n",
    "            f1_avg = f1_score(test_dset.targets, test_slide_avg_m, average='macro')\n",
    "            f1_max = f1_score(test_dset.targets, test_slide_max_m, average='macro')\n",
    "            f1_t10 = f1_score(test_dset.targets, test_slide_avgt10_m, average='macro')\n",
    "            # balanced accuracy\n",
    "            bacc_mv = balanced_accuracy_score(test_dset.targets, test_slide_mjvt)\n",
    "            bacc_avg = balanced_accuracy_score(test_dset.targets, test_slide_avg_m)\n",
    "            bacc_max = balanced_accuracy_score(test_dset.targets, test_slide_max_m)\n",
    "            bacc_avgt10 = balanced_accuracy_score(test_dset.targets, test_slide_avgt10_m)\n",
    "            # AUC\n",
    "\n",
    "            auc_test_avg = evaluator.compute_auc(test_dset.targets, test_slide_avg)\n",
    "            auc_test_max = evaluator.compute_auc(test_dset.targets, test_slide_max)\n",
    "            auc_test_top10 = evaluator.compute_auc(test_dset.targets, test_slide_avgt10)\n",
    "      \n",
    "            print(\"\\n----------------------Test Set Results-------------------------------\")\n",
    "            print(f\"Test F1-scores: MV={f1_mv:.3f}, AVG={f1_avg:.3f}, MAX={f1_max:.3f}, top10={f1_t10:.3f}\")\n",
    "            print(f\"Test Balanced Acc: MV={bacc_mv:.3f}, AVG={bacc_avg:.3f}, MAX={bacc_max:.3f}, top10={bacc_avgt10:.3f}\")\n",
    "            print(f\"Test AUC-scores: AVG={auc_test_avg:.3f}, MAX={auc_test_max:.3f}, top10={auc_test_top10:.3f}\")\n",
    "            # confusion matrix of average predictions only\n",
    "            print(\"Confusion Matrix (Average Predictions):\")\n",
    "            print(confusion_matrix(test_dset.targets, test_slide_avg_m))\n",
    "            # classification report of average predictions only\n",
    "            print(\"Classification Report (Average Predictions):\")\n",
    "            print(classification_report(test_dset.targets, test_slide_avg_m))\n",
    "            print(\"------------------------------------------------------\\n\")\n",
    "            # save the ground truth for test slides\n",
    "            df1 = pd.DataFrame({\n",
    "                'Case_ID': test_dset.cases,\n",
    "                'WSI_Id': test_dset.slides,\n",
    "                'n_tiles': test_dset.ntiles,\n",
    "                'label_desc': test_dset.label_desc,\n",
    "                'label_id': test_dset.targets\n",
    "            })\n",
    "            df1.to_csv(os.path.join(args.output, 'test_GT.csv'), index=False)\n",
    "            # save the average predictions of the test slides\n",
    "            df2 = pd.DataFrame({\n",
    "                'wsi_id': test_dset.slides,\n",
    "                'nonMSI_prob': test_slide_avg[:, 0],\n",
    "                'MSI_prob': test_slide_avg[:, 1]\n",
    "            })\n",
    "            df2.to_csv(os.path.join(args.output, 'test_pred_avg_AUC.csv'), index=False)\n",
    "            # save the max predictions of the test slides\n",
    "            df3 = pd.DataFrame({\n",
    "                'wsi_id': test_dset.slides,\n",
    "                'nonMSI_prob': test_slide_max[:, 0],\n",
    "                'MSI_prob': test_slide_max[:, 1]\n",
    "            })\n",
    "            df3.to_csv(os.path.join(args.output, 'test_pred_max_AUC.csv'), index=False)\n",
    "            # save the top10 predictions of the test slides\n",
    "            df4 = pd.DataFrame({\n",
    "                'wsi_id': test_dset.slides,\n",
    "                'nonMSI_prob': test_slide_avgt10[:, 0],\n",
    "                'MSI_prob': test_slide_avgt10[:, 1]\n",
    "            })\n",
    "            df4.to_csv(os.path.join(args.output, 'test_pred_t10_AUC.csv'), index=False)\n",
    "            # save original tile level predictions for test slides\n",
    "            df5 = pd.DataFrame({\n",
    "                'slideidx': test_dset.slideIDX,\n",
    "                'nonMSI_prob': test_probs[:, 0],\n",
    "                'MSI_prob': test_probs[:, 1],\n",
    "                'pred_tile': test_preds\n",
    "            })\n",
    "            df5.to_csv(os.path.join(args.output, 'test_tile_pred_AUC.csv'), index=False)\n",
    "            print('..............Test set done ...............')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
