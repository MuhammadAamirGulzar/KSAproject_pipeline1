{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual results verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import  custom_label_binarize\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, balanced_accuracy_score, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(r\"E:\\KSA Project\\Project_Pipeline\\existing_approaches\\CAIMAN_Fivecrop_4Folds\\MSI_vs_MSS_T50R50\\fold4\\val_GT.csv\")\n",
    "patch_predictions = pd.read_csv(r\"E:\\KSA Project\\Project_Pipeline\\existing_approaches\\CAIMAN_Fivecrop_4Folds\\MSI_vs_MSS_T50R50\\fold4\\val_tile_pred_AUC.csv\")\n",
    "# ground_truth = pd.read_csv(r\"E:\\KSA Project\\Project_Pipeline\\existing_approaches\\CAIMAN_Fivecrop_4Folds\\MSI_vs_MSS_T50R50\\fold4\\test_GT.csv\")\n",
    "# patch_predictions = pd.read_csv(r\"E:\\KSA Project\\Project_Pipeline\\existing_approaches\\CAIMAN_Fivecrop_4Folds\\MSI_vs_MSS_T50R50\\fold4\\test_tile_pred_AUC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for top10_youdens_j method:\n",
      "  threshold: 0.8864\n",
      "  accuracy: 0.8100\n",
      "  balanced_accuracy: 0.7510\n",
      "  weighted_f1: 0.8266\n",
      "  macro_f1: 0.6974\n",
      "  roc_auc: 0.7937\n",
      "Metrics for top10_max_f1 method:\n",
      "  threshold: 0.9857\n",
      "  accuracy: 0.8900\n",
      "  balanced_accuracy: 0.6882\n",
      "  weighted_f1: 0.8754\n",
      "  macro_f1: 0.7298\n",
      "  roc_auc: 0.7937\n",
      "Metrics for avg_youdens_j method:\n",
      "  threshold: 0.0763\n",
      "  accuracy: 0.8500\n",
      "  balanced_accuracy: 0.8020\n",
      "  weighted_f1: 0.8610\n",
      "  macro_f1: 0.7513\n",
      "  roc_auc: 0.8329\n",
      "Metrics for avg_max_f1 method:\n",
      "  threshold: 0.0763\n",
      "  accuracy: 0.8500\n",
      "  balanced_accuracy: 0.8020\n",
      "  weighted_f1: 0.8610\n",
      "  macro_f1: 0.7513\n",
      "  roc_auc: 0.8329\n",
      "Metrics for max_youdens_j method:\n",
      "  threshold: 0.8956\n",
      "  accuracy: 0.7000\n",
      "  balanced_accuracy: 0.7686\n",
      "  weighted_f1: 0.7426\n",
      "  macro_f1: 0.6280\n",
      "  roc_auc: 0.7663\n",
      "Metrics for max_max_f1 method:\n",
      "  threshold: 0.8956\n",
      "  accuracy: 0.7000\n",
      "  balanced_accuracy: 0.7686\n",
      "  weighted_f1: 0.7426\n",
      "  macro_f1: 0.6280\n",
      "  roc_auc: 0.7663\n",
      "Metrics for majority_youdens_j method:\n",
      "  threshold: 0.0099\n",
      "  accuracy: 0.7100\n",
      "  balanced_accuracy: 0.8020\n",
      "  weighted_f1: 0.7513\n",
      "  macro_f1: 0.6442\n",
      "  roc_auc: 0.8427\n",
      "Metrics for majority_max_f1 method:\n",
      "  threshold: 0.0620\n",
      "  accuracy: 0.8500\n",
      "  balanced_accuracy: 0.8020\n",
      "  weighted_f1: 0.8610\n",
      "  macro_f1: 0.7513\n",
      "  roc_auc: 0.8427\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Merge patch predictions with ground truth labels\n",
    "ground_truth[\"slideidx\"] = range(len(ground_truth))  # Assuming slideidx is aligned with ground truth order\n",
    "merged_df = patch_predictions.merge(ground_truth, on=\"slideidx\", how=\"left\")\n",
    "\n",
    "# Group data by slide index\n",
    "grouped = merged_df.groupby(\"slideidx\")\n",
    "\n",
    "# Initialize empty lists for aggregated probabilities\n",
    "top10_probs = []\n",
    "avg_probs = []\n",
    "max_probs = []\n",
    "majority_probs = []\n",
    "labels = []\n",
    "\n",
    "# Aggregation methods\n",
    "for slideidx, group in grouped:\n",
    "    msi_probs = group[\"MSI_prob\"].values\n",
    "    nonmsi_probs = group[\"nonMSI_prob\"].values\n",
    "    true_label = group[\"label_id\"].iloc[0]\n",
    "    \n",
    "    # Top10 MSI probabilities\n",
    "    top10_msi_prob = np.mean(np.sort(msi_probs)[-10:]) if len(msi_probs) >= 10 else np.mean(msi_probs)\n",
    "    top10_probs.append(top10_msi_prob)\n",
    "    \n",
    "    # Average MSI probabilities\n",
    "    avg_msi_prob = np.mean(msi_probs)\n",
    "    avg_probs.append(avg_msi_prob)\n",
    "    \n",
    "    # Maximum MSI probability\n",
    "    max_msi_prob = np.max(msi_probs)\n",
    "    max_probs.append(max_msi_prob)\n",
    "    \n",
    "    # Majority vote MSI probability\n",
    "    majority_msi_prob = np.sum(msi_probs > 0.5) / len(msi_probs)\n",
    "    majority_probs.append(majority_msi_prob)\n",
    "    \n",
    "    # Append true label\n",
    "    labels.append(true_label)\n",
    "\n",
    "# Save aggregated results in a new DataFrame\n",
    "aggregated_results = pd.DataFrame({\n",
    "    \"slideidx\": grouped.groups.keys(),\n",
    "    \"top10_msi_prob\": top10_probs,\n",
    "    \"avg_msi_prob\": avg_probs,\n",
    "    \"max_msi_prob\": max_probs,\n",
    "    \"majority_msi_prob\": majority_probs,\n",
    "    \"true_label\": labels\n",
    "})\n",
    "\n",
    "# Calculate evaluation metrics for each method\n",
    "def calculate_thresholds(y_true, y_probs):\n",
    "    \"\"\"\n",
    "    Calculate optimal thresholds using various methods.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): True binary labels.\n",
    "        y_probs (np.ndarray): Predicted probabilities.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of thresholds for each method.\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "\n",
    "    # 1. Youden's J statistic\n",
    "    youdens_j = tpr - fpr\n",
    "    youdens_threshold = thresholds[np.argmax(youdens_j)]\n",
    "\n",
    "    # 2. Maximizing F1-Score\n",
    "    best_f1_threshold = None\n",
    "    best_f1 = -1\n",
    "    for threshold in thresholds:\n",
    "        preds = (y_probs >= threshold).astype(int)\n",
    "        f1 = f1_score(y_true, preds)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_f1_threshold = threshold\n",
    "\n",
    "    return {\n",
    "        \"youdens_j\": youdens_threshold,\n",
    "        \"max_f1\": best_f1_threshold,\n",
    "    }\n",
    "metrics = {}\n",
    "\n",
    "for method in [\"top10\", \"avg\", \"max\", \"majority\"]:\n",
    "    probs = aggregated_results[f\"{method}_msi_prob\"].values\n",
    "    true_labels = aggregated_results[\"true_label\"].values\n",
    "\n",
    "    # Calculate optimal thresholds\n",
    "    thresholds = calculate_thresholds(true_labels, probs)\n",
    "    \n",
    "    # Evaluate metrics for each threshold\n",
    "    for name, threshold in thresholds.items():\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        metrics[f\"{method}_{name}\"] = {\n",
    "            \"threshold\": threshold,\n",
    "            \"accuracy\": accuracy_score(true_labels, preds),\n",
    "            \"balanced_accuracy\": balanced_accuracy_score(true_labels, preds),\n",
    "            \"weighted_f1\": f1_score(true_labels, preds, average=\"weighted\"),\n",
    "            \"macro_f1\": f1_score(true_labels, preds, average=\"macro\"),\n",
    "            \"roc_auc\": roc_auc_score(true_labels, probs),\n",
    "        }\n",
    "\n",
    "# Print metrics\n",
    "for method, method_metrics in metrics.items():\n",
    "    print(f\"Metrics for {method} method:\")\n",
    "    for metric, value in method_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# save the results to a file \n",
    "aggregated_results.to_csv(r\"E:\\KSA Project\\Project_Pipeline\\existing_approaches\\CAIMAN_Fivecrop_4Folds\\MSI_vs_MSS_T50R50\\fold4\\aggregated_slide_predictions.csv\", index=False)\n",
    "print(\"done\")\n",
    "pd.DataFrame(metrics).T.to_csv(r\"E:\\KSA Project\\Project_Pipeline\\existing_approaches\\CAIMAN_Fivecrop_4Folds\\MSI_vs_MSS_T50R50\\fold4\\aggregated_slide_metrics.csv\", index=True)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
