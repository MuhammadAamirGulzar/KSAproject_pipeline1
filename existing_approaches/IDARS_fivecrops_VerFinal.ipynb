{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the finalised IDARS pipeline using the FiveCrop transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import auc, roc_curve,roc_auc_score, f1_score, precision_recall_curve, average_precision_score, classification_report \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, balanced_accuracy_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import cv2\n",
    "from scipy.stats.mstats import gmean\n",
    "from custom_tranformations import RandomRotation\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from utils import encode_onehot, custom_label_binarize\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('CUDA is not available. Using CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                             Global Configurations                             #\n",
    "################################################################################\n",
    "\n",
    "KFOLD_PATH = r\"E:\\Aamir Gulzar\\dataset\\splits\\kfolds_IDARS.csv\"\n",
    "DATA_PATH = r\"E:\\Aamir Gulzar\\dataset\\patches\"\n",
    "\n",
    "# GPU memory limit (commented out in your code)\n",
    "# torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "\n",
    "# Some global metrics placeholders\n",
    "lmbda = 0.1\n",
    "best_auc_v = 0\n",
    "best_auc = 0\n",
    "n_slides = 0\n",
    "best_loss = 100000.\n",
    "best_f1_v = 0.\n",
    "best_Acc = 0.\n",
    "best_ap_v = 0.\n",
    "\n",
    "num_classes = 2\n",
    "label_names = ['nonMSI', 'MSI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--budget'], dest='budget', nargs=None, const=None, default=0.8, type=<class 'float'>, choices=None, required=False, help='the budget for how often the network can get hints', metavar='N')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='MSI And MSS Classification')\n",
    "parser.add_argument('--data_lib', type=str, default=KFOLD_PATH, help='path to train ')\n",
    "parser.add_argument('--val_lib', type=str, default=KFOLD_PATH, help='path to validation ')\n",
    "parser.add_argument('--test_lib', type=str, default=KFOLD_PATH, help='path to validation ')\n",
    "parser.add_argument('--problem', type=str, default='MSI_vs_MSS_T1R10', help='classification problem.')\n",
    "parser.add_argument('--pos_label', type=int, default=1, help='positive label.')\n",
    "parser.add_argument('--neg_label', type=int, default=0, help='negative label. If present.')\n",
    "parser.add_argument('--output', type=str, default='IDaRS_Fivecrop_4Folds', help='now i am using t=1 and r=10')\n",
    "parser.add_argument('--folds', type=int, default=4, help='number of fold to execute')\n",
    "parser.add_argument('--batch_size', type=int, default=512, help='mini-batch size (default: 128)')\n",
    "parser.add_argument('--nepochs', type=int, default=4, help='number of epochs')\n",
    "parser.add_argument('--workers', default=0, type=int, help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--test_every', default=1, type=int, help='test on val every (default: 10)')\n",
    "parser.add_argument('--weights', default=0.5, type=float, help='unbalanced positive class weight (default: 0.5, balanced classes)')\n",
    "\n",
    "parser.add_argument('--lr', type=float, default=0.001) # best 0.01\n",
    "parser.add_argument('--l2_reg', type=float, default=5e-3)\n",
    "parser.add_argument('--grad_bound', type=float, default=5.0)\n",
    "\n",
    "parser.add_argument('--r', default=10, type=int, help='how many rand tiles to consider (default: 10)')\n",
    "parser.add_argument('--k', default=1, type=int, help='how many top k tiles to consider (default: 10)')\n",
    "\n",
    "parser.add_argument('--budget', type=float, default=0.8, metavar='N',\n",
    "                    help='the budget for how often the network can get hints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                     Aggregation Class (for tile-level merges)                #\n",
    "################################################################################\n",
    "\n",
    "class Aggregation:\n",
    "    \"\"\"\n",
    "    This class holds all tile-level aggregation methods. \n",
    "    It can be configured via 'aggregation_config' to enable or disable certain\n",
    "    aggregations or to conditionally save results to CSV, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "    @staticmethod\n",
    "    def group_argtopk(groups, data, k=1):\n",
    "        \"\"\"\n",
    "        Return the indices of the top-k elements by group.\n",
    "        \"\"\"\n",
    "        order = np.lexsort((data, groups))\n",
    "        groups = groups[order]\n",
    "        data = data[order]\n",
    "        index = np.empty(len(groups), 'bool')\n",
    "        index[-k:] = True\n",
    "        index[:-k] = groups[k:] != groups[:-k]\n",
    "        return list(order[index])\n",
    "\n",
    "    @staticmethod\n",
    "    def group_max(groups, data, nmax):\n",
    "        \"\"\"\n",
    "        Return the maximum value in each group. \n",
    "        nmax is the maximum group ID + 1, used to fill results in an array.\n",
    "        \"\"\"\n",
    "        out = np.empty(nmax)\n",
    "        out[:] = np.nan\n",
    "        order = np.lexsort((data, groups))\n",
    "        groups = groups[order]\n",
    "        data = data[order]\n",
    "        index = np.empty(len(groups), 'bool')\n",
    "        index[-1] = True\n",
    "        index[:-1] = groups[1:] != groups[:-1]\n",
    "        out[groups[index]] = data[index]\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def group_avg(groups, data):\n",
    "        \"\"\"\n",
    "        Compute mean of data for each unique group.\n",
    "        \"\"\"\n",
    "        order = np.lexsort((data, groups))\n",
    "        groups = groups[order]\n",
    "        data = data[order]\n",
    "        unames, idx, counts = np.unique(groups, return_inverse=True, return_counts=True)\n",
    "        sum_pred = np.bincount(idx, weights=data)\n",
    "        mean_pred = sum_pred / counts\n",
    "        return mean_pred, sum_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def get_binnedtopK_aggregation(group, data):\n",
    "        \"\"\"\n",
    "        Use top-1, mean, top percentages, \n",
    "        plus a custom top-10 threshold-based score.\n",
    "        (We keep this if you still want a separate top aggregator.)\n",
    "        \"\"\"\n",
    "        wsi_dict = {}\n",
    "        for idx, g in enumerate(group):\n",
    "            if g not in wsi_dict:\n",
    "                wsi_dict[g] = [data[idx]]\n",
    "            else:\n",
    "                wsi_dict[g].append(data[idx])\n",
    "\n",
    "        topk_p = []  \n",
    "        top10_sc = []  \n",
    "        for each_wsi in wsi_dict.keys():\n",
    "            aggregated = []\n",
    "            wsi_predictions = np.array(wsi_dict[each_wsi], dtype='float64')\n",
    "            wsi_predictions.sort()\n",
    "\n",
    "            # Keep top1 and average\n",
    "            aggregated.append(wsi_predictions[-1])          # top-1\n",
    "            aggregated.append(np.mean(wsi_predictions))     # avg\n",
    "            for k_in in [1, 4]:                             # leftover from original\n",
    "                temp = wsi_predictions[-k_in:]\n",
    "                aggregated.append(np.mean(temp))\n",
    "\n",
    "            topk_p.append(np.mean(aggregated))\n",
    "\n",
    "            # For the top 10 predictions\n",
    "            top10_predictions = wsi_predictions[-10:]\n",
    "            if sum(top10_predictions[top10_predictions > 0.5]) > 0:\n",
    "                a = np.mean(top10_predictions[top10_predictions > 0.5])\n",
    "            else:\n",
    "                a = 0\n",
    "            if sum(top10_predictions[top10_predictions < 0.5]) > 0:\n",
    "                b = np.mean(top10_predictions[top10_predictions < 0.5])\n",
    "            else:\n",
    "                b = 0\n",
    "            top10_sc.append((a + a + b) / 3)\n",
    "\n",
    "        topk_p = np.array(topk_p, dtype='float64')\n",
    "        top10_sc = np.array(top10_sc, dtype='float64')\n",
    "        return topk_p, top10_sc\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_aggregated_probabilities(group, data):\n",
    "        \"\"\"\n",
    "        **Reduced** aggregator that returns only avg & max\n",
    "        for each group. \n",
    "        \"\"\"\n",
    "        wsi_dict = {}\n",
    "        for idx, g in enumerate(group):\n",
    "            if g not in wsi_dict:\n",
    "                wsi_dict[g] = [data[idx]]\n",
    "            else:\n",
    "                wsi_dict[g].append(data[idx])\n",
    "\n",
    "        avg_p = []\n",
    "        max_p = []\n",
    "        # (Removed sum, median, gmean, top-half-mean)\n",
    "\n",
    "        for each_wsi in wsi_dict.keys():\n",
    "            wsi_predictions = np.array(wsi_dict[each_wsi], dtype='float64')\n",
    "            avg_p.append(np.mean(wsi_predictions))  # keep only average\n",
    "            max_p.append(np.max(wsi_predictions))   # keep only max\n",
    "\n",
    "        avg_p = np.array(avg_p, dtype='float64')\n",
    "        max_p = np.array(max_p, dtype='float64')\n",
    "        return avg_p, max_p\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_aggregated_predictions(group, data):\n",
    "        \"\"\"\n",
    "        Compute majority-vote predictions for each WSI. \n",
    "        \"\"\"\n",
    "        wsi_dict = {}\n",
    "        for idx, g in enumerate(group):\n",
    "            if g not in wsi_dict:\n",
    "                wsi_dict[g] = [data[idx]]\n",
    "            else:\n",
    "                wsi_dict[g].append(data[idx])\n",
    "\n",
    "        mv_pred = []\n",
    "        n_pred = []\n",
    "        for each_wsi in wsi_dict.keys():\n",
    "            wsi_predictions = wsi_dict[each_wsi]\n",
    "            wsi_pred_class = []\n",
    "            for cl in range(num_classes):\n",
    "                wsi_pred_class.append(wsi_predictions.count(cl))\n",
    "\n",
    "            mj_vt = np.argmax(wsi_pred_class)\n",
    "            mv_pred.append(mj_vt)\n",
    "            n_pred.append(wsi_pred_class)\n",
    "\n",
    "        n_pred = np.array(n_pred, dtype='float64')\n",
    "        mv_pred = np.array(mv_pred, dtype='float64')\n",
    "        return mv_pred, n_pred\n",
    "\n",
    "    @staticmethod\n",
    "    def group_avg_df(groups, data):\n",
    "        \"\"\"\n",
    "        Group top-10 aggregator using pandas. \n",
    "        Returns the mean of nlargest(10) for each group.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame({'Slide': groups, 'value': data})\n",
    "        group_average_df = df.groupby('Slide')['value'].apply(lambda grp: grp.nlargest(10).mean())\n",
    "        group_average = group_average_df.tolist()\n",
    "        return group_average\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topMedtraining(group, data):\n",
    "        \"\"\"\n",
    "        Optional median-based aggregator (not used in main pipeline).\n",
    "        \"\"\"\n",
    "        wsi_dict = {}\n",
    "        for idx, g in enumerate(group):\n",
    "            if g not in wsi_dict:\n",
    "                wsi_dict[g] = [data[idx]]\n",
    "            else:\n",
    "                wsi_dict[g].append(data[idx])\n",
    "\n",
    "        top_p = []\n",
    "        for each_wsi in wsi_dict.keys():\n",
    "            wsi_predictions = np.array(wsi_dict[each_wsi], dtype='float64')\n",
    "            md = np.median(wsi_predictions)\n",
    "            start_i = np.squeeze(np.argwhere(group == each_wsi))[0]\n",
    "            indices = np.squeeze(np.argwhere(wsi_predictions >= md)) + start_i\n",
    "            top_p.extend(indices)\n",
    "\n",
    "        top_p = np.array(top_p, dtype='int64')\n",
    "        return top_p\n",
    "\n",
    "    @staticmethod\n",
    "    def get_topKtraining(group, data, k=5):\n",
    "        \"\"\"\n",
    "        Return the indices of the top-k% probabilities for each WSI group.\n",
    "        \"\"\"\n",
    "        wsi_dict = {}\n",
    "        for idx, g in enumerate(group):\n",
    "            if g not in wsi_dict:\n",
    "                wsi_dict[g] = [data[idx]]\n",
    "            else:\n",
    "                wsi_dict[g].append(data[idx])\n",
    "\n",
    "        topk_p = []\n",
    "        for each_wsi in wsi_dict.keys():\n",
    "            wsi_predictions = np.array(wsi_dict[each_wsi], dtype='float64')\n",
    "            start_i = np.squeeze(np.argwhere(group == each_wsi)[0])\n",
    "            perc = int((k / 100) * len(wsi_predictions)) + 1\n",
    "            topk = wsi_predictions.argsort()[-perc:]\n",
    "            topk_p.extend(topk + start_i)\n",
    "\n",
    "        topk_p = np.squeeze(np.array(topk_p, dtype='int64'))\n",
    "        return topk_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                     Evaluation Class (metrics, plotting, etc.)               #\n",
    "################################################################################\n",
    "\n",
    "class Evaluation:\n",
    "    \"\"\"\n",
    "    This class holds evaluation-related methods such as metrics calculation,\n",
    "    confusion matrices, classification reports, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        if config is None:\n",
    "            config = {}\n",
    "        self.config = config\n",
    "\n",
    "    @staticmethod\n",
    "    def cutoff_youdens_j(fpr, tpr, thresholds):\n",
    "        j_scores = tpr - fpr\n",
    "        j_ordered = sorted(zip(j_scores, thresholds))\n",
    "        return j_ordered[-1][1]\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_f1_score(targets, prediction, cutoff):\n",
    "        prediction = np.array(prediction)\n",
    "        targets = np.array(targets)\n",
    "        return f1_score(targets, prediction, average='weighted')\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_metrics(target, prediction):\n",
    "        fpr, tpr, thresholds = roc_curve(target, prediction)\n",
    "        cutoff = Evaluation.cutoff_youdens_j(fpr, tpr, thresholds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        f1score = Evaluation.cal_f1_score(target, prediction, cutoff)\n",
    "        precision, recall, _ = precision_recall_curve(target, prediction, zero_division=1)\n",
    "        average_precision = average_precision_score(target, prediction, zero_division=1)\n",
    "        return f1score, average_precision, roc_auc, cutoff\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_accuracy(output, target):\n",
    "        preds = output.max(1, keepdim=True)[1]\n",
    "        correct = preds.eq(target.view_as(preds)).sum()\n",
    "        acc = correct.float() / preds.shape[0]\n",
    "        return acc\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_auc(labels, predictions):\n",
    "        # binary_labels = custom_label_binarize(np.array(labels), classes=[i for i in range(num_classes)])\n",
    "        # auc_list = []\n",
    "        # for cl in range(num_classes):\n",
    "        #     fpr, tpr, thresholds = roc_curve(binary_labels[:, cl], predictions[:, cl])\n",
    "        #     auc_list.append(auc(fpr, tpr))\n",
    "        aucscore = roc_auc_score(labels, predictions[:, 1])\n",
    "        return aucscore\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_metrics(target, prediction, set):\n",
    "        import matplotlib.pyplot as plt\n",
    "        fpr, tpr, thresholds = roc_curve(target, prediction)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print('roc_auc is:', roc_auc)\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(target, prediction, zero_division=1)\n",
    "        average_precision = average_precision_score(target, prediction, zero_division=1)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        lw = 2\n",
    "\n",
    "        # Subplot 1: ROC\n",
    "        plt.subplot(121)\n",
    "        plt.plot(fpr, tpr, color='darkorange',\n",
    "                 lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve, AUC={0:0.2f}'.format(roc_auc))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "        # Subplot 2: Precision-Recall\n",
    "        plt.subplot(122)\n",
    "        plt.step(recall, precision, alpha=0.4, color='darkorange', where='post')\n",
    "        plt.fill_between(recall, precision, alpha=0.2, color='navy', step='post')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.ylim([0, 1.05])\n",
    "        plt.xlim([0, 1])\n",
    "        plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
    "\n",
    "        plt.savefig(os.path.join(args.output, 'roc_pr' + set + '.png'))\n",
    "        plt.close(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                            Training/Testing Utilities                        #\n",
    "################################################################################\n",
    "\n",
    "def inference(run, loader, model, criterion):\n",
    "    model.eval()\n",
    "    running_acc = 0.\n",
    "    running_loss = 0.\n",
    "    probs = torch.FloatTensor(len(loader.dataset), num_classes)\n",
    "    preds = torch.FloatTensor(len(loader.dataset))\n",
    "\n",
    "    index_offset = 0  # --- 5-crop change ---\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, target) in enumerate(loader):\n",
    "            # inputs shape: (batch_size, 5, 3, 224, 224) for 5-crop\n",
    "            b, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w).cuda()     # shape: (batch_size*5, 3, 224, 224)\n",
    "            target = target.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(inputs)                       # shape: (batch_size*5, num_classes)\n",
    "\n",
    "            # Average logits over the 5 crops to get a single prediction per sample\n",
    "            output = output.view(b, ncrops, -1).mean(1)  # shape: (batch_size, num_classes)\n",
    "            loss = criterion(output, target)\n",
    "            acc = Evaluation.calculate_accuracy(output, target)\n",
    "            y = F.softmax(output, dim=-1)\n",
    "            pred_value, pred = torch.max(output.data, 1)\n",
    "\n",
    "            # Fill in our large preds/probs arrays\n",
    "            batch_size_now = b\n",
    "            preds[index_offset:index_offset + batch_size_now] = pred.detach().clone()\n",
    "            probs[index_offset:index_offset + batch_size_now] = y.detach().clone()\n",
    "            index_offset += batch_size_now\n",
    "\n",
    "            running_acc += acc.item() * batch_size_now\n",
    "            running_loss += loss.item() * batch_size_now\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('Inference\\tEpoch: [{:3d}/{:3d}]\\tBatch: [{:3d}/{}]\\tValidation Loss: {:.4f}, acc: {:0.2f}%'\n",
    "                      .format(run + 1, args.nepochs, i + 1, len(loader),\n",
    "                              running_loss / ((i + 1) * batch_size_now),\n",
    "                              (100 * running_acc) / ((i + 1) * batch_size_now)))\n",
    "    return (\n",
    "        probs.cpu().numpy(),\n",
    "        running_loss / len(loader.dataset),\n",
    "        running_acc / len(loader.dataset),\n",
    "        preds.cpu().numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "def train(run, loader, model, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    index_offset = 0  # --- 5-crop change ---\n",
    "    for i, (inputs, target) in enumerate(loader):\n",
    "        # inputs shape: (batch_size, 5, 3, 224, 224) for 5-crop\n",
    "        # print('5. i am running upto here inside train \\n\\n')\n",
    "        b, ncrops, c, h, w = inputs.shape\n",
    "        inputs = inputs.view(-1, c, h, w).cuda()  # shape: (batch_size*5, 3, 224, 224)\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)                    # shape: (batch_size*5, num_classes)\n",
    "        # print('6. i am running upto here model given me output \\n\\n')\n",
    "        # Average over 5 crops\n",
    "        output = output.view(b, ncrops, -1).mean(1)  # shape: (batch_size, num_classes)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * b\n",
    "        acc = Evaluation.calculate_accuracy(output, target)\n",
    "        running_acc += acc.item() * b\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Train Epoch: [{:3d}/{:3d}] Batch: {:3d}, Training Loss: {:.4f}, Acc: {:.2f}%\"\n",
    "                  .format(run + 1, args.nepochs, i + 1,\n",
    "                          running_loss / ((i + 1) * b),\n",
    "                          100 * running_acc / ((i + 1) * b)))\n",
    "\n",
    "    return (running_loss / len(loader.dataset),\n",
    "            running_acc / len(loader.dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                                Dataset Class                                 #\n",
    "################################################################################\n",
    "def get_split_indices(lib, test_fold, val_fold):\n",
    "    \"\"\"\n",
    "    lib: DataFrame with a column named 'fold' \n",
    "         that has values in [0, 1, 2, 3].\n",
    "    test_fold, val_fold: integers from 0..3\n",
    "    returns: three lists => train_idx, val_idx, test_idx\n",
    "    \"\"\"\n",
    "    # Extract all folds\n",
    "    all_folds = lib['fold'].values.tolist()\n",
    "    test_idx = [i for i, f in enumerate(all_folds) if f == test_fold]\n",
    "    val_idx  = [i for i, f in enumerate(all_folds) if f == val_fold]\n",
    "    train_idx = [i for i, f in enumerate(all_folds)\n",
    "                 if f not in (test_fold, val_fold)]\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "class MILdataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset to handle multiple WSI tiles from a library file (CSV).\n",
    "    Each tile is read at 512×512 and then transformed into 5-crop (224×224).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, libraryfile=KFOLD_PATH, transform=None, mult=2, s=10, \n",
    "                 shuffle=False, set='', test_fold=None, val_fold=None):\n",
    "        path_dir = DATA_PATH\n",
    "        lib = pd.DataFrame(pd.read_csv(libraryfile, usecols=[\n",
    "            'Case_ID', 'WSI_Dir', 'label_desc', 'label_id', f'fold'\n",
    "        ]))\n",
    "\n",
    "        lib.dropna(inplace=True)\n",
    "\n",
    "        allcases = lib['Case_ID'].values.tolist()\n",
    "        allslides = lib['WSI_Dir'].values.tolist()\n",
    "        tar = lib['label_id'].values.tolist()\n",
    "        label_desc = lib['label_desc'].values.tolist()\n",
    "        split = lib[f'fold'].values.tolist()\n",
    "\n",
    "        train_idx, val_idx, test_idx = get_split_indices(lib, test_fold, val_fold)\n",
    "        if set == 'train':\n",
    "            print(f\"##--Targets ==> {len(tar)} | {tar.count(0)} | {tar.count(1)} --##\")\n",
    "            indices = train_idx\n",
    "            print(f\"##--Train Split ==> {len(indices)} Slides\")\n",
    "            thresh_tiles=3\n",
    "        elif set == 'valid':\n",
    "            indices = val_idx\n",
    "            print(f\"##--Val Split ==> {len(indices)} Slides\")\n",
    "            thresh_tiles=3\n",
    "        elif set == 'test':\n",
    "            indices = test_idx\n",
    "            print(f\"##--Test Split ==> {len(indices)} Slides\")\n",
    "            thresh_tiles=3\n",
    "        else:\n",
    "            raise ValueError(\"Invalid set_type. Must be 'train', 'valid', or 'test'.\")\n",
    "\n",
    "        cases = []\n",
    "        tiles = []\n",
    "        ntiles = []\n",
    "        slideIDX = []\n",
    "        targetIDX = []\n",
    "        targets = []\n",
    "        label_desciption = []\n",
    "        slides = []\n",
    "\n",
    "        j = 0\n",
    "        for i in indices:\n",
    "            path = os.path.join(path_dir, str(allslides[i]))\n",
    "            slide_label = int(tar[i])\n",
    "            if os.path.exists(path):\n",
    "                # max patch per slide comment it if you want to use all patches or make MAX_PATCHES = len(t)\n",
    "                # MAX_PATCHES = 25\n",
    "                t = [\n",
    "                    os.path.join(path, f) \n",
    "                    for f in os.listdir(path) \n",
    "                    if f.endswith('.png')\n",
    "                ]\n",
    "                if len(t) >= thresh_tiles:\n",
    "                    cases.append(allcases[i])\n",
    "                    slides.append(allslides[i])\n",
    "                    tiles.extend(t)\n",
    "                    ntiles.append(len(t))\n",
    "                    slideIDX.extend([j]*len(t))\n",
    "                    targetIDX.extend([slide_label]*len(t))\n",
    "                    targets.append(slide_label)\n",
    "                    label_desciption.append(label_desc[i])\n",
    "                    j += 1\n",
    "\n",
    "        print('-------------------------')\n",
    "        print('Number of Slides: {}'.format(len(slides)))\n",
    "        print('Number of tiles: {}'.format(len(tiles)))\n",
    "        print('Max tiles: ', max(ntiles) if len(ntiles) else 0)\n",
    "        print('Min tiles: ', min(ntiles) if len(ntiles) else 0)\n",
    "        print('Average tiles: ', np.mean(ntiles) if len(ntiles) else 0)\n",
    "        print('nonMSI: ', targets.count(0))\n",
    "        print('MSI: ', targets.count(1))\n",
    "\n",
    "        self.slideIDX = slideIDX\n",
    "        self.ntiles = ntiles\n",
    "        self.tiles = tiles\n",
    "        self.targets = targets\n",
    "        self.label_desc = label_desciption\n",
    "        self.slides = slides\n",
    "        self.cases = cases\n",
    "\n",
    "        self.transform = transform\n",
    "        self.mult = mult\n",
    "        self.s = s\n",
    "        self.mode = None\n",
    "        self.shuffle = shuffle\n",
    "        self.targetIDX = targetIDX\n",
    "\n",
    "    def setmode(self, mode):\n",
    "        self.mode = mode\n",
    "\n",
    "    def maketraindata(self, idxs):\n",
    "        self.t_data = [(self.slideIDX[x], self.tiles[x], self.targets[self.slideIDX[x]]) \n",
    "                       for x in idxs]\n",
    "\n",
    "    def shuffletraindata(self):\n",
    "        self.t_data = random.sample(self.t_data, len(self.t_data))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.mode == 1:\n",
    "            tile = self.tiles[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "\n",
    "            # --- 5-crop change ---\n",
    "            # Remove manual resize; rely on self.transform pipeline\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)  # => shape: (5, 3, 224, 224)\n",
    "            # 'img' is now a 5-crop tensor\n",
    "\n",
    "            slide_idx = self.slideIDX[index]\n",
    "            target = self.targets[slide_idx]\n",
    "            return img, target\n",
    "\n",
    "        elif self.mode == 2:\n",
    "            slideIDX, tile, target = self.t_data[index]\n",
    "            img = Image.open(str(tile)).convert('RGB')\n",
    "\n",
    "            # --- 5-crop change ---\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)  # => shape: (5, 3, 224, 224)\n",
    "\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == 1:\n",
    "            return len(self.tiles)\n",
    "        elif self.mode == 2:\n",
    "            return len(self.t_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--Targets ==> 405 | 344 | 61 --##\n",
      "##--Train Split ==> 211 Slides\n",
      "-------------------------\n",
      "Number of Slides: 211\n",
      "Number of tiles: 131716\n",
      "Max tiles:  1875\n",
      "Min tiles:  16\n",
      "Average tiles:  624.2464454976304\n",
      "nonMSI:  178\n",
      "MSI:  33\n",
      "##--Val Split ==> 94 Slides\n",
      "-------------------------\n",
      "Number of Slides: 94\n",
      "Number of tiles: 55618\n",
      "Max tiles:  1682\n",
      "Min tiles:  6\n",
      "Average tiles:  591.6808510638298\n",
      "nonMSI:  81\n",
      "MSI:  13\n",
      "##--Test Split ==> 100 Slides\n",
      "-------------------------\n",
      "Number of Slides: 100\n",
      "Number of tiles: 55849\n",
      "Max tiles:  1942\n",
      "Min tiles:  30\n",
      "Average tiles:  558.49\n",
      "nonMSI:  85\n",
      "MSI:  15\n",
      "211\n",
      "Number of TopK 1426\n",
      "Number of randk k 13276\n",
      "Number of randk and topk combined  14702\n",
      "Train Epoch: [  1/  4] Batch:   1, Training Loss: 0.5857, Acc: 75.78%\n",
      "Training\tEpoch: [1/4]\tLoss: 0.4176\tAccuracy:  83\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/29]\tValidation Loss: 0.3652, acc: 84.57%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/109]\tValidation Loss: 0.9187, acc: 56.25%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [101/109]\tValidation Loss: 0.5306, acc: 77.13%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.589  AVG=0.589  MAX=0.561  top10=0.548\n",
      "Balanced Acc (Val): MV=0.592  AVG=0.592  MAX=0.587  top10=0.561\n",
      "AUC-scores (Val): AVG=0.683 MAX=0.534 top10=0.531\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Saved checkpoint_best_AUC.pth\n",
      "Number of randk k 13276\n",
      "Number of randk and topk combined  13560\n",
      "Train Epoch: [  2/  4] Batch:   1, Training Loss: 0.3710, Acc: 85.35%\n",
      "Training\tEpoch: [2/4]\tLoss: 0.3506\tAccuracy:  85\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/27]\tValidation Loss: 0.1078, acc: 98.24%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/109]\tValidation Loss: 0.0643, acc: 99.80%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [101/109]\tValidation Loss: 0.3656, acc: 90.75%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.463  AVG=0.463  MAX=0.460  top10=0.460\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.494  top10=0.494\n",
      "AUC-scores (Val): AVG=0.531 MAX=0.491 top10=0.470\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Number of randk k 13276\n",
      "Number of randk and topk combined  13549\n",
      "Train Epoch: [  3/  4] Batch:   1, Training Loss: 0.3078, Acc: 88.67%\n",
      "Training\tEpoch: [3/4]\tLoss: 0.3192\tAccuracy:  87\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/27]\tValidation Loss: 0.2665, acc: 89.65%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/109]\tValidation Loss: 0.2561, acc: 91.80%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [101/109]\tValidation Loss: 0.4442, acc: 85.64%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.570  AVG=0.631  MAX=0.552  top10=0.598\n",
      "Balanced Acc (Val): MV=0.558  AVG=0.603  MAX=0.546  top10=0.585\n",
      "AUC-scores (Val): AVG=0.648 MAX=0.550 top10=0.556\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Number of randk k 13276\n",
      "Number of randk and topk combined  13549\n",
      "Train Epoch: [  4/  4] Batch:   1, Training Loss: 0.2929, Acc: 88.28%\n",
      "Training\tEpoch: [4/4]\tLoss: 0.2903\tAccuracy:  87\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/27]\tValidation Loss: 0.1107, acc: 99.22%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/109]\tValidation Loss: 0.0901, acc: 99.61%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/109]\tValidation Loss: 0.3388, acc: 90.37%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.463  AVG=0.463  MAX=0.460  top10=0.460\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.494  top10=0.494\n",
      "AUC-scores (Val): AVG=0.677 MAX=0.619 top10=0.623\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/110]\tValidation Loss: 1.0732, acc: 64.26%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/110]\tValidation Loss: 0.6350, acc: 72.76%\n",
      "Predicted Tiles: 55849, classes: 2, total targets: 100\n",
      "\n",
      "----------------------Test Set Results-------------------------------\n",
      "Test F1-scores: MV=0.574, AVG=0.574, MAX=0.562, top10=0.526\n",
      "Test Balanced Acc: MV=0.565, AVG=0.565, MAX=0.588, top10=0.533\n",
      "Test AUC-scores: AVG=0.694, MAX=0.669, top10=0.679\n",
      "Confusion Matrix (Average Predictions):\n",
      "[[79  6]\n",
      " [12  3]]\n",
      "Classification Report (Average Predictions):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90        85\n",
      "           1       0.33      0.20      0.25        15\n",
      "\n",
      "    accuracy                           0.82       100\n",
      "   macro avg       0.60      0.56      0.57       100\n",
      "weighted avg       0.79      0.82      0.80       100\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "..............Test set done ...............\n",
      "##--Targets ==> 405 | 344 | 61 --##\n",
      "##--Train Split ==> 200 Slides\n",
      "-------------------------\n",
      "Number of Slides: 200\n",
      "Number of tiles: 119899\n",
      "Max tiles:  1942\n",
      "Min tiles:  18\n",
      "Average tiles:  599.495\n",
      "nonMSI:  172\n",
      "MSI:  28\n",
      "##--Val Split ==> 111 Slides\n",
      "-------------------------\n",
      "Number of Slides: 111\n",
      "Number of tiles: 67666\n",
      "Max tiles:  1849\n",
      "Min tiles:  16\n",
      "Average tiles:  609.6036036036036\n",
      "nonMSI:  91\n",
      "MSI:  20\n",
      "##--Test Split ==> 94 Slides\n",
      "-------------------------\n",
      "Number of Slides: 94\n",
      "Number of tiles: 55618\n",
      "Max tiles:  1682\n",
      "Min tiles:  6\n",
      "Average tiles:  591.6808510638298\n",
      "nonMSI:  81\n",
      "MSI:  13\n",
      "200\n",
      "Number of TopK 1300\n",
      "Number of randk k 12091\n",
      "Number of randk and topk combined  13391\n",
      "Train Epoch: [  1/  4] Batch:   1, Training Loss: 1.0388, Acc: 21.29%\n",
      "Training\tEpoch: [1/4]\tLoss: 0.4674\tAccuracy:  77\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/27]\tValidation Loss: 0.6852, acc: 72.07%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/133]\tValidation Loss: 0.0908, acc: 98.63%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [101/133]\tValidation Loss: 0.8475, acc: 77.06%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.450  AVG=0.450  MAX=0.546  top10=0.546\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.550  top10=0.550\n",
      "AUC-scores (Val): AVG=0.677 MAX=0.684 top10=0.657\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Saved checkpoint_best_AUC.pth\n",
      "Number of randk k 12091\n",
      "Number of randk and topk combined  12357\n",
      "Train Epoch: [  2/  4] Batch:   1, Training Loss: 0.4015, Acc: 84.38%\n",
      "Training\tEpoch: [2/4]\tLoss: 0.3319\tAccuracy:  87\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/25]\tValidation Loss: 0.6109, acc: 73.44%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/133]\tValidation Loss: 0.3375, acc: 86.33%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [101/133]\tValidation Loss: 0.6585, acc: 75.10%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.588  AVG=0.588  MAX=0.722  top10=0.692\n",
      "Balanced Acc (Val): MV=0.575  AVG=0.575  MAX=0.698  top10=0.659\n",
      "AUC-scores (Val): AVG=0.792 MAX=0.776 top10=0.761\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Saved checkpoint_best_AUC.pth\n",
      "Number of randk k 12091\n",
      "Number of randk and topk combined  12349\n",
      "Train Epoch: [  3/  4] Batch:   1, Training Loss: 0.3131, Acc: 88.48%\n",
      "Training\tEpoch: [3/4]\tLoss: 0.2974\tAccuracy:  88\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/25]\tValidation Loss: 0.5003, acc: 76.95%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/133]\tValidation Loss: 0.0583, acc: 100.00%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [101/133]\tValidation Loss: 0.9813, acc: 76.12%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.450  AVG=0.450  MAX=0.495  top10=0.500\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.520  top10=0.525\n",
      "AUC-scores (Val): AVG=0.637 MAX=0.634 top10=0.624\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Number of randk k 12091\n",
      "Number of randk and topk combined  12348\n",
      "Train Epoch: [  4/  4] Batch:   1, Training Loss: 0.3250, Acc: 86.72%\n",
      "Training\tEpoch: [4/4]\tLoss: 0.2813\tAccuracy:  88\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/25]\tValidation Loss: 0.5505, acc: 74.41%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/133]\tValidation Loss: 0.1500, acc: 97.07%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/133]\tValidation Loss: 0.7077, acc: 75.58%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.450  AVG=0.450  MAX=0.527  top10=0.539\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.534  top10=0.545\n",
      "AUC-scores (Val): AVG=0.760 MAX=0.741 top10=0.733\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/109]\tValidation Loss: 0.1922, acc: 94.73%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/109]\tValidation Loss: 0.4125, acc: 87.29%\n",
      "Predicted Tiles: 55618, classes: 2, total targets: 94\n",
      "\n",
      "----------------------Test Set Results-------------------------------\n",
      "Test F1-scores: MV=0.537, AVG=0.537, MAX=0.580, top10=0.570\n",
      "Test Balanced Acc: MV=0.538, AVG=0.538, MAX=0.565, top10=0.558\n",
      "Test AUC-scores: AVG=0.550, MAX=0.538, top10=0.499\n",
      "Confusion Matrix (Average Predictions):\n",
      "[[81  0]\n",
      " [12  1]]\n",
      "Classification Report (Average Predictions):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93        81\n",
      "           1       1.00      0.08      0.14        13\n",
      "\n",
      "    accuracy                           0.87        94\n",
      "   macro avg       0.94      0.54      0.54        94\n",
      "weighted avg       0.89      0.87      0.82        94\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "..............Test set done ...............\n",
      "##--Targets ==> 405 | 344 | 61 --##\n",
      "##--Train Split ==> 194 Slides\n",
      "-------------------------\n",
      "Number of Slides: 194\n",
      "Number of tiles: 111467\n",
      "Max tiles:  1942\n",
      "Min tiles:  6\n",
      "Average tiles:  574.5721649484536\n",
      "nonMSI:  166\n",
      "MSI:  28\n",
      "##--Val Split ==> 100 Slides\n",
      "-------------------------\n",
      "Number of Slides: 100\n",
      "Number of tiles: 64050\n",
      "Max tiles:  1875\n",
      "Min tiles:  18\n",
      "Average tiles:  640.5\n",
      "nonMSI:  87\n",
      "MSI:  13\n",
      "##--Test Split ==> 111 Slides\n",
      "-------------------------\n",
      "Number of Slides: 111\n",
      "Number of tiles: 67666\n",
      "Max tiles:  1849\n",
      "Min tiles:  16\n",
      "Average tiles:  609.6036036036036\n",
      "nonMSI:  91\n",
      "MSI:  20\n",
      "194\n",
      "Number of TopK 1211\n",
      "Number of randk k 11248\n",
      "Number of randk and topk combined  12459\n",
      "Train Epoch: [  1/  4] Batch:   1, Training Loss: 0.4082, Acc: 86.33%\n",
      "Training\tEpoch: [1/4]\tLoss: 0.3302\tAccuracy:  87\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/25]\tValidation Loss: 0.6320, acc: 72.66%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/126]\tValidation Loss: 0.0397, acc: 99.80%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [101/126]\tValidation Loss: 0.8249, acc: 82.60%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.465  AVG=0.465  MAX=0.465  top10=0.465\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.500  top10=0.500\n",
      "AUC-scores (Val): AVG=0.618 MAX=0.694 top10=0.706\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Saved checkpoint_best_AUC.pth\n",
      "Number of randk k 11248\n",
      "Number of randk and topk combined  11497\n",
      "Train Epoch: [  2/  4] Batch:   1, Training Loss: 0.2719, Acc: 89.65%\n",
      "Training\tEpoch: [2/4]\tLoss: 0.2768\tAccuracy:  89\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/23]\tValidation Loss: 0.3992, acc: 77.73%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/126]\tValidation Loss: 0.0662, acc: 99.41%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [101/126]\tValidation Loss: 0.8077, acc: 82.46%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.465  AVG=0.465  MAX=0.465  top10=0.465\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.500  top10=0.500\n",
      "AUC-scores (Val): AVG=0.611 MAX=0.658 top10=0.661\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Number of randk k 11248\n",
      "Number of randk and topk combined  11493\n",
      "Train Epoch: [  3/  4] Batch:   1, Training Loss: 0.2187, Acc: 91.60%\n",
      "Training\tEpoch: [3/4]\tLoss: 0.2515\tAccuracy:  90\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/23]\tValidation Loss: 0.4680, acc: 78.12%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/126]\tValidation Loss: 0.0520, acc: 99.80%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [101/126]\tValidation Loss: 0.7878, acc: 82.05%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.539  AVG=0.539  MAX=0.646  top10=0.660\n",
      "Balanced Acc (Val): MV=0.538  AVG=0.538  MAX=0.610  top10=0.615\n",
      "AUC-scores (Val): AVG=0.640 MAX=0.683 top10=0.681\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Number of randk k 11248\n",
      "Number of randk and topk combined  11492\n",
      "Train Epoch: [  4/  4] Batch:   1, Training Loss: 0.3008, Acc: 87.11%\n",
      "Training\tEpoch: [4/4]\tLoss: 0.2370\tAccuracy:  90\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/23]\tValidation Loss: 0.3825, acc: 80.47%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/126]\tValidation Loss: 0.0861, acc: 99.22%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/126]\tValidation Loss: 0.7388, acc: 79.93%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.462  AVG=0.462  MAX=0.634  top10=0.539\n",
      "Balanced Acc (Val): MV=0.494  AVG=0.494  MAX=0.604  top10=0.538\n",
      "AUC-scores (Val): AVG=0.625 MAX=0.681 top10=0.687\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/133]\tValidation Loss: 0.1352, acc: 96.09%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/133]\tValidation Loss: 0.8690, acc: 76.58%\n",
      "Predicted Tiles: 67666, classes: 2, total targets: 111\n",
      "\n",
      "----------------------Test Set Results-------------------------------\n",
      "Test F1-scores: MV=0.450, AVG=0.450, MAX=0.450, top10=0.450\n",
      "Test Balanced Acc: MV=0.500, AVG=0.500, MAX=0.500, top10=0.500\n",
      "Test AUC-scores: AVG=0.742, MAX=0.729, top10=0.734\n",
      "Confusion Matrix (Average Predictions):\n",
      "[[91  0]\n",
      " [20  0]]\n",
      "Classification Report (Average Predictions):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        91\n",
      "           1       0.00      0.00      0.00        20\n",
      "\n",
      "    accuracy                           0.82       111\n",
      "   macro avg       0.41      0.50      0.45       111\n",
      "weighted avg       0.67      0.82      0.74       111\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "..............Test set done ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\tcga\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3\\envs\\tcga\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Anaconda3\\envs\\tcga\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##--Targets ==> 405 | 344 | 61 --##\n",
      "##--Train Split ==> 205 Slides\n",
      "-------------------------\n",
      "Number of Slides: 205\n",
      "Number of tiles: 123284\n",
      "Max tiles:  1849\n",
      "Min tiles:  6\n",
      "Average tiles:  601.3853658536585\n",
      "nonMSI:  172\n",
      "MSI:  33\n",
      "##--Val Split ==> 100 Slides\n",
      "-------------------------\n",
      "Number of Slides: 100\n",
      "Number of tiles: 55849\n",
      "Max tiles:  1942\n",
      "Min tiles:  30\n",
      "Average tiles:  558.49\n",
      "nonMSI:  85\n",
      "MSI:  15\n",
      "##--Test Split ==> 100 Slides\n",
      "-------------------------\n",
      "Number of Slides: 100\n",
      "Number of tiles: 64050\n",
      "Max tiles:  1875\n",
      "Min tiles:  18\n",
      "Average tiles:  640.5\n",
      "nonMSI:  87\n",
      "MSI:  13\n",
      "205\n",
      "Number of TopK 1337\n",
      "Number of randk k 12433\n",
      "Number of randk and topk combined  13770\n",
      "Train Epoch: [  1/  4] Batch:   1, Training Loss: 1.4279, Acc: 14.06%\n",
      "Training\tEpoch: [1/4]\tLoss: 0.5282\tAccuracy:  73\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/27]\tValidation Loss: 0.4750, acc: 81.05%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [  1/110]\tValidation Loss: 0.9157, acc: 78.71%\n",
      "Inference\tEpoch: [  1/  4]\tBatch: [101/110]\tValidation Loss: 0.4999, acc: 81.97%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.459  AVG=0.459  MAX=0.459  top10=0.459\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.500  top10=0.500\n",
      "AUC-scores (Val): AVG=0.801 MAX=0.711 top10=0.722\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Saved checkpoint_best_AUC.pth\n",
      "Number of randk k 12433\n",
      "Number of randk and topk combined  12700\n",
      "Train Epoch: [  2/  4] Batch:   1, Training Loss: 0.3321, Acc: 87.50%\n",
      "Training\tEpoch: [2/4]\tLoss: 0.3200\tAccuracy:  87\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/25]\tValidation Loss: 0.4229, acc: 83.01%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [  1/110]\tValidation Loss: 0.5587, acc: 78.71%\n",
      "Inference\tEpoch: [  2/  4]\tBatch: [101/110]\tValidation Loss: 0.5532, acc: 81.58%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.459  AVG=0.459  MAX=0.459  top10=0.459\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.500  top10=0.500\n",
      "AUC-scores (Val): AVG=0.771 MAX=0.755 top10=0.730\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Number of randk k 12433\n",
      "Number of randk and topk combined  12694\n",
      "Train Epoch: [  3/  4] Batch:   1, Training Loss: 0.2939, Acc: 87.89%\n",
      "Training\tEpoch: [3/4]\tLoss: 0.2877\tAccuracy:  88\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/25]\tValidation Loss: 0.3811, acc: 85.94%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [  1/110]\tValidation Loss: 0.3640, acc: 78.91%\n",
      "Inference\tEpoch: [  3/  4]\tBatch: [101/110]\tValidation Loss: 0.5438, acc: 81.96%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.459  AVG=0.459  MAX=0.591  top10=0.564\n",
      "Balanced Acc (Val): MV=0.500  AVG=0.500  MAX=0.576  top10=0.555\n",
      "AUC-scores (Val): AVG=0.766 MAX=0.787 top10=0.776\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Number of randk k 12433\n",
      "Number of randk and topk combined  12694\n",
      "Train Epoch: [  4/  4] Batch:   1, Training Loss: 0.2904, Acc: 87.89%\n",
      "Training\tEpoch: [4/4]\tLoss: 0.2687\tAccuracy:  89\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/25]\tValidation Loss: 0.3466, acc: 85.16%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/110]\tValidation Loss: 0.8813, acc: 71.48%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/110]\tValidation Loss: 0.5342, acc: 79.53%\n",
      "\n",
      "----------------------Validation Results -------------------------------------\n",
      "F1-scores (Val): MV=0.582  AVG=0.524  MAX=0.681  top10=0.667\n",
      "Balanced Acc (Val): MV=0.567  AVG=0.533  MAX=0.649  top10=0.627\n",
      "AUC-scores (Val): AVG=0.790 MAX=0.815 top10=0.798\n",
      "------------------------------------------------------------------\n",
      "\n",
      "Saved checkpoint_best_F1.pth\n",
      "Saved checkpoint_best_AUC.pth\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [  1/126]\tValidation Loss: 0.2676, acc: 88.67%\n",
      "Inference\tEpoch: [  4/  4]\tBatch: [101/126]\tValidation Loss: 0.6857, acc: 77.27%\n",
      "Predicted Tiles: 64050, classes: 2, total targets: 100\n",
      "\n",
      "----------------------Test Set Results-------------------------------\n",
      "Test F1-scores: MV=0.582, AVG=0.622, MAX=0.621, top10=0.622\n",
      "Test Balanced Acc: MV=0.565, AVG=0.598, MAX=0.614, top10=0.598\n",
      "Test AUC-scores: AVG=0.664, MAX=0.695, top10=0.684\n",
      "Confusion Matrix (Average Predictions):\n",
      "[[84  3]\n",
      " [10  3]]\n",
      "Classification Report (Average Predictions):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93        87\n",
      "           1       0.50      0.23      0.32        13\n",
      "\n",
      "    accuracy                           0.87       100\n",
      "   macro avg       0.70      0.60      0.62       100\n",
      "weighted avg       0.84      0.87      0.85       100\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "..............Test set done ...............\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################################\n",
    "#                                Main Pipeline                                 #\n",
    "################################################################################\n",
    "class StackAndNormalize:\n",
    "    def __init__(self, normalize):\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, crops):\n",
    "        return torch.stack([\n",
    "            self.normalize(transforms.ToTensor()(crop)) \n",
    "            for crop in crops\n",
    "        ])\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training/validation/test pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    global args, best_auc_v, tr_batch_size, n_slides, best_auc, best_f1_v\n",
    "\n",
    "    config = {\n",
    "        'aggregation_methods': ['average', 'top10', 'max'],\n",
    "        'save_val_tile_csv': True,\n",
    "        'save_test_tile_csv': True,\n",
    "        'save_val_slide_csv': True,\n",
    "        'save_test_slide_csv': True,\n",
    "        'enable_group_avg_df': True,\n",
    "        'enable_plot_metrics': True,\n",
    "        'enable_confusion_matrix': True\n",
    "    }\n",
    "\n",
    "    aggregator = Aggregation(config=config)\n",
    "    evaluator = Evaluation(config=config)\n",
    "\n",
    "    args = parser.parse_args(\"\")\n",
    "    if not os.path.exists(args.output):\n",
    "        os.mkdir(args.output)\n",
    "\n",
    "    args.output = os.path.join(args.output, args.problem)\n",
    "    if not os.path.exists(args.output):\n",
    "        os.mkdir(args.output)\n",
    "\n",
    "    temp_output = args.output\n",
    "\n",
    "    AUC_SCORES = []\n",
    "    F1_SCORES = []\n",
    "\n",
    "    for fold in range(args.folds):\n",
    "        test_fold = fold + 1\n",
    "        val_fold  = ((fold + 1) % 4) + 1\n",
    "        args.output = os.path.join(temp_output, 'fold' + str(fold + 1))\n",
    "        if not os.path.exists(args.output):\n",
    "            os.mkdir(args.output)\n",
    "        path_fold = args.output\n",
    "\n",
    "        for sets in range(1):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            args.output = os.path.join(path_fold, 'best' + str(sets))\n",
    "            if not os.path.exists(args.output):\n",
    "                os.mkdir(args.output)\n",
    "\n",
    "            global best_auc_v, best_auc, n_slides, best_loss, best_f1_v, best_Acc, best_ap_v\n",
    "            best_auc_v = 0\n",
    "            best_auc = 0\n",
    "            n_slides = 0\n",
    "            best_loss = 100000.\n",
    "            best_f1_v = 0.\n",
    "            best_Acc = 0.\n",
    "            best_ap_v = 0.\n",
    "\n",
    "            model = models.resnet34(weights='DEFAULT')\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "            model.cuda()\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "            cudnn.benchmark = True\n",
    "            criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "               # --- 5-crop change: define transforms so we return 5-crops (224×224) for each 512×512 image\n",
    "            normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                                std=[0.1, 0.1, 0.1])\n",
    "            trans = transforms.Compose([\n",
    "                RandomRotation([0, 90, 180, 270]),\n",
    "                transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.05),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "                transforms.FiveCrop(224),   # returns tuple of 5 PIL images\n",
    "                StackAndNormalize(normalize)])\n",
    "\n",
    "            trans_Valid = transforms.Compose([\n",
    "                transforms.FiveCrop(224),\n",
    "                StackAndNormalize(normalize)])\n",
    "            # --- end 5-crop changes\n",
    "\n",
    "            # Load Data\n",
    "            train_dset = MILdataset(args.data_lib, trans, set='train',  test_fold=test_fold, val_fold=val_fold)\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_dset,\n",
    "                batch_size=args.batch_size, shuffle=False,\n",
    "                num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "            val_dset = MILdataset(args.data_lib, trans_Valid, set='valid',  test_fold=test_fold, val_fold=val_fold)\n",
    "            val_loader = torch.utils.data.DataLoader(\n",
    "                val_dset,\n",
    "                batch_size=args.batch_size, shuffle=False,\n",
    "                num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "            test_dset = MILdataset(args.data_lib, trans_Valid, set='test',  test_fold=test_fold, val_fold=val_fold)\n",
    "            test_loader = torch.utils.data.DataLoader(\n",
    "                test_dset,\n",
    "                batch_size=args.batch_size, shuffle=False,\n",
    "                num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "            #  if you wanted to see only dataset distribution and data loaders etc then put continue here\n",
    "            # continue\n",
    "            fconv = open(os.path.join(args.output, 'train_convergence.csv'), 'w')\n",
    "            fconv.write('epoch,loss,accuracy\\n')\n",
    "            fconv.close()\n",
    "\n",
    "            fconv = open(os.path.join(args.output, 'valid_convergence.csv'), 'w')\n",
    "            # We removed sum, median, gmean columns; only keep relevant columns\n",
    "            fconv.write('epoch,tile_loss,tile_acc,best_F1,F1_AVG,F1_Max,F1_T10,best_BAcc,Bacc_AVG,Bacc_Max,Bacc_T10,best_AUC,Avg_AUC,Max_AUC,Top_AUC\\n')\n",
    "            fconv.close()\n",
    "\n",
    "            num_tiles = len(train_dset.slideIDX)\n",
    "            n_slides = len(train_dset.slides)\n",
    "            print(n_slides)\n",
    "\n",
    "            topk = aggregator.get_topKtraining(np.array(train_dset.slideIDX), np.random.rand(num_tiles), args.k)\n",
    "\n",
    "            print('Number of TopK', len(topk))\n",
    "            # print('1. i am running upto here before epoch loop \\n\\n')\n",
    "            for epoch in range(args.nepochs):\n",
    "                train_dset.setmode(1)\n",
    "                # print('2. i am running upto here inside epoch loop before topk \\n\\n')\n",
    "                randk = aggregator.get_topKtraining(np.array(train_dset.slideIDX), np.random.rand(num_tiles), args.r)\n",
    "                print('Number of randk k', len(randk))\n",
    "                randk = list(randk) + list(topk)\n",
    "                print('Number of randk and topk combined ', len(randk))\n",
    "                # print('3. i am running upto here before make train \\n\\n')\n",
    "                train_dset.maketraindata(randk)\n",
    "                train_dset.shuffletraindata()\n",
    "                train_dset.setmode(2)\n",
    "                # print('4. i am running upto here before train \\n\\n')\n",
    "                loss, acc = train(epoch, train_loader, model, criterion, optimizer)\n",
    "                print('Training\\tEpoch: [{}/{}]\\tLoss: {:0.4f}\\tAccuracy: {:3d}'\n",
    "                      .format(epoch+1, args.nepochs, loss, int(acc * 100)))\n",
    "\n",
    "                fconv = open(os.path.join(args.output, 'train_convergence.csv'), 'a')\n",
    "                fconv.write('{},{:0.4f},{:3d}\\n'.format(epoch, loss, int(acc * 100)))\n",
    "                fconv.close()\n",
    "\n",
    "                # Inference on same subset\n",
    "                train_dset.maketraindata(randk)\n",
    "                trn_probs, _, _, _ = inference(epoch, train_loader, model, criterion)\n",
    "\n",
    "                slide_idx = [train_dset.slideIDX[item] for item in randk]\n",
    "                randk = np.array(randk, dtype='int64')\n",
    "\n",
    "                top_prob = 1. - trn_probs[:, 0]\n",
    "                topk = aggregator.get_topKtraining(np.array(slide_idx), top_prob, args.k)\n",
    "                topk = np.array(topk, dtype='int64')\n",
    "                topk = [randk[item] for item in topk]\n",
    "\n",
    "                # Validation\n",
    "                if (epoch + 1) % args.test_every == 0:\n",
    "                    val_dset.setmode(1)\n",
    "                    val_probs, val_loss, val_acc, val_preds = inference(epoch, val_loader, model, criterion)\n",
    "                    val_slide_mjvt, _ = aggregator.compute_aggregated_predictions(\n",
    "                        np.array(val_dset.slideIDX), val_preds\n",
    "                    )\n",
    "\n",
    "                    # We only keep avg, max, top10\n",
    "                    val_slide_avg = []\n",
    "                    val_slide_max = []\n",
    "                    val_slide_avgt10 = []\n",
    "\n",
    "                    # We'll store each class aggregator\n",
    "                    for cl in range(num_classes):\n",
    "                        t_avg, t_max = aggregator.compute_aggregated_probabilities(\n",
    "                            np.array(val_dset.slideIDX), val_probs[:, cl]\n",
    "                        )\n",
    "                        t_t10 = aggregator.group_avg_df(\n",
    "                            np.array(val_dset.slideIDX), val_probs[:, cl]\n",
    "                        )\n",
    "                        val_slide_avg.append(t_avg)\n",
    "                        val_slide_max.append(t_max)\n",
    "                        val_slide_avgt10.append(t_t10)\n",
    "\n",
    "                    val_slide_avg = np.array(val_slide_avg).transpose()\n",
    "                    val_slide_max = np.array(val_slide_max).transpose()\n",
    "                    val_slide_avgt10 = np.array(val_slide_avgt10).transpose()\n",
    "\n",
    "                    val_slide_avg_m = np.argmax(val_slide_avg, axis=1)\n",
    "                    val_slide_max_m = np.argmax(val_slide_max, axis=1)\n",
    "                    val_slide_avgt10_m = np.argmax(val_slide_avgt10, axis=1)\n",
    "\n",
    "                    from sklearn.metrics import f1_score\n",
    "                    f1_mv = f1_score(val_dset.targets, val_slide_mjvt, average='macro')\n",
    "                    f1_avg = f1_score(val_dset.targets, val_slide_avg_m, average='macro')\n",
    "                    f1_max = f1_score(val_dset.targets, val_slide_max_m, average='macro')\n",
    "                    f1_a10 = f1_score(val_dset.targets, val_slide_avgt10_m, average='macro')\n",
    "                    bacc_mv = balanced_accuracy_score(val_dset.targets, val_slide_mjvt)\n",
    "                    bacc_avg = balanced_accuracy_score(val_dset.targets, val_slide_avg_m)\n",
    "                    bacc_max = balanced_accuracy_score(val_dset.targets, val_slide_max_m)\n",
    "                    bacc_avgt10 = balanced_accuracy_score(val_dset.targets, val_slide_avgt10_m)\n",
    "\n",
    "                    auc_val_avg = evaluator.compute_auc(val_dset.targets, val_slide_avg)\n",
    "                    auc_val_max = evaluator.compute_auc(val_dset.targets, val_slide_max)\n",
    "                    auc_val_top10 = evaluator.compute_auc(val_dset.targets, val_slide_avgt10)\n",
    "\n",
    "                    fconv = open(os.path.join(args.output, 'valid_convergence.csv'), 'a')\n",
    "                    fconv.write('{},{:0.4f},{:3d},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f},{:0.4f}\\n'.format(\n",
    "                        epoch, val_loss, int(val_acc * 100),\n",
    "                        max(f1_mv,f1_avg, f1_max, f1_a10),\n",
    "                        f1_avg, f1_max, f1_a10,\n",
    "                        # save max and individual balanced accuracies\n",
    "                        max(bacc_mv, bacc_avg, bacc_max, bacc_avgt10),\n",
    "                        bacc_avg, bacc_max, bacc_avgt10,\n",
    "                        # also save best auc values\n",
    "                        max(auc_val_avg, auc_val_max, auc_val_top10),\n",
    "                        auc_val_avg, auc_val_max, auc_val_top10\n",
    "                    ))\n",
    "                    fconv.close()\n",
    "\n",
    "                    print(\"\\n----------------------Validation Results -------------------------------------\")\n",
    "                    print(f\"F1-scores (Val): MV={f1_mv:.3f}  AVG={f1_avg:.3f}  MAX={f1_max:.3f}  top10={f1_a10:.3f}\")\n",
    "                    print(f\"Balanced Acc (Val): MV={bacc_mv:.3f}  AVG={bacc_avg:.3f}  MAX={bacc_max:.3f}  top10={bacc_avgt10:.3f}\")\n",
    "                    print(f\"AUC-scores (Val): AVG={auc_val_avg:.3f} MAX={auc_val_max:.3f} top10={auc_val_top10:.3f}\")\n",
    "                    print(\"------------------------------------------------------------------\\n\")\n",
    "\n",
    "                    best_f1_candidate = max(f1_mv, f1_avg, f1_max, f1_a10)\n",
    "                    if best_f1_candidate > best_f1_v:\n",
    "                        best_f1_v = best_f1_candidate\n",
    "                        obj = {\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_ap_v': best_f1_v,\n",
    "                            'best_auc_v': max(auc_val_max, auc_val_top10, auc_val_avg),\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "                        torch.save(obj, os.path.join(args.output, 'checkpoint_best_F1.pth'))\n",
    "                        print(\"Saved checkpoint_best_F1.pth\")\n",
    "                        # save the tile level predictions for validation slides columns slideidx, prob non msi, prob msi and pred tile\n",
    "                        df2_f1 = pd.DataFrame({\n",
    "                            'slideidx': val_dset.slideIDX,\n",
    "                            'nonMSI_prob': val_probs[:, 0],\n",
    "                            'MSI_prob': val_probs[:, 1],\n",
    "                            'pred_tile': val_preds\n",
    "                        })\n",
    "                        df2_f1.to_csv(os.path.join(args.output, 'val_tile_pred_F1.csv'), index=False)\n",
    "\n",
    "                    best_auc_candidate = max(auc_val_avg, auc_val_max, auc_val_top10)\n",
    "                    if best_auc_candidate > best_auc_v:\n",
    "                        best_auc_v = best_auc_candidate\n",
    "                        obj = {\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_ap_v': best_f1_v,\n",
    "                            'best_auc_v': best_auc_v,\n",
    "                            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "                        torch.save(obj, os.path.join(args.output, 'checkpoint_best_AUC.pth'))\n",
    "                        print(\"Saved checkpoint_best_AUC.pth\")\n",
    "\n",
    "                        # save the tile level predictions for validation slides columns slideidx, prob non msi, prob msi and pred tile\n",
    "                        df2_auc = pd.DataFrame({\n",
    "                            'slideidx': val_dset.slideIDX,\n",
    "                            'nonMSI_prob': val_probs[:, 0],\n",
    "                            'MSI_prob': val_probs[:, 1],\n",
    "                            'pred_tile': val_preds\n",
    "                        })\n",
    "                        df2_auc.to_csv(os.path.join(args.output, 'val_tile_pred_AUC.csv'), index=False)\n",
    "\n",
    "            # Save ground truth for validation slides\n",
    "            df1 = pd.DataFrame({\n",
    "                'Case_ID': val_dset.cases,\n",
    "                'WSI_Id': val_dset.slides,\n",
    "                'n_tiles': val_dset.ntiles,\n",
    "                'label_desc': val_dset.label_desc,\n",
    "                'label_id': val_dset.targets\n",
    "            })\n",
    "            df1.to_csv(os.path.join(args.output, 'val_GT.csv'), index=False)\n",
    "            df2_f1.to_csv(os.path.join(args.output, 'val_tile_pred_F1.csv'), index=False)\n",
    "            df2_auc.to_csv(os.path.join(args.output, 'val_tile_pred_AUC.csv'), index=False)\n",
    "        \n",
    "            ############## Test ##############\n",
    "            ch = torch.load(os.path.join(args.output, 'checkpoint_best_AUC.pth'))\n",
    "            model.load_state_dict(ch['state_dict'])\n",
    "\n",
    "            test_dset.setmode(1)\n",
    "            test_probs, test_loss, test_acc, test_preds = inference(epoch, test_loader, model, criterion)\n",
    "\n",
    "            print(f'Predicted Tiles: {len(test_probs[:, 1])}, classes: {len(test_probs[0, :])}, total targets: {len(test_dset.targets)}')\n",
    "\n",
    "            test_slide_mjvt, _ = aggregator.compute_aggregated_predictions(\n",
    "                np.array(test_dset.slideIDX), test_preds\n",
    "            )\n",
    "            test_slide_avg = []\n",
    "            test_slide_max = []\n",
    "            test_slide_avgt10 = []\n",
    "            for cl in range(num_classes):\n",
    "                t_avg, t_max = aggregator.compute_aggregated_probabilities(\n",
    "                    np.array(test_dset.slideIDX), test_probs[:, cl]\n",
    "                )\n",
    "                t_t10 = aggregator.group_avg_df(\n",
    "                    np.array(test_dset.slideIDX), test_probs[:, cl]\n",
    "                )\n",
    "                test_slide_avg.append(t_avg)\n",
    "                test_slide_max.append(t_max)\n",
    "                test_slide_avgt10.append(t_t10)\n",
    "\n",
    "            test_slide_avg = np.array(test_slide_avg).transpose()\n",
    "            test_slide_max = np.array(test_slide_max).transpose()\n",
    "            test_slide_avgt10 = np.array(test_slide_avgt10).transpose()\n",
    "\n",
    "            test_slide_avg_m = np.argmax(test_slide_avg, axis=1)\n",
    "            test_slide_max_m = np.argmax(test_slide_max, axis=1)\n",
    "            test_slide_avgt10_m = np.argmax(test_slide_avgt10, axis=1)\n",
    "\n",
    "            f1_mv = f1_score(test_dset.targets, test_slide_mjvt, average='macro')\n",
    "            f1_avg = f1_score(test_dset.targets, test_slide_avg_m, average='macro')\n",
    "            f1_max = f1_score(test_dset.targets, test_slide_max_m, average='macro')\n",
    "            f1_t10 = f1_score(test_dset.targets, test_slide_avgt10_m, average='macro')\n",
    "            # balanced accuracy\n",
    "            bacc_mv = balanced_accuracy_score(test_dset.targets, test_slide_mjvt)\n",
    "            bacc_avg = balanced_accuracy_score(test_dset.targets, test_slide_avg_m)\n",
    "            bacc_max = balanced_accuracy_score(test_dset.targets, test_slide_max_m)\n",
    "            bacc_avgt10 = balanced_accuracy_score(test_dset.targets, test_slide_avgt10_m)\n",
    "            # AUC\n",
    "\n",
    "            auc_test_avg = evaluator.compute_auc(test_dset.targets, test_slide_avg)\n",
    "            auc_test_max = evaluator.compute_auc(test_dset.targets, test_slide_max)\n",
    "            auc_test_top10 = evaluator.compute_auc(test_dset.targets, test_slide_avgt10)\n",
    "      \n",
    "            print(\"\\n----------------------Test Set Results-------------------------------\")\n",
    "            print(f\"Test F1-scores: MV={f1_mv:.3f}, AVG={f1_avg:.3f}, MAX={f1_max:.3f}, top10={f1_t10:.3f}\")\n",
    "            print(f\"Test Balanced Acc: MV={bacc_mv:.3f}, AVG={bacc_avg:.3f}, MAX={bacc_max:.3f}, top10={bacc_avgt10:.3f}\")\n",
    "            print(f\"Test AUC-scores: AVG={auc_test_avg:.3f}, MAX={auc_test_max:.3f}, top10={auc_test_top10:.3f}\")\n",
    "            # confusion matrix of average predictions only\n",
    "            print(\"Confusion Matrix (Average Predictions):\")\n",
    "            print(confusion_matrix(test_dset.targets, test_slide_avg_m))\n",
    "            # classification report of average predictions only\n",
    "            print(\"Classification Report (Average Predictions):\")\n",
    "            print(classification_report(test_dset.targets, test_slide_avg_m))\n",
    "            print(\"------------------------------------------------------\\n\")\n",
    "            # save the ground truth for test slides\n",
    "            df1 = pd.DataFrame({\n",
    "                'Case_ID': test_dset.cases,\n",
    "                'WSI_Id': test_dset.slides,\n",
    "                'n_tiles': test_dset.ntiles,\n",
    "                'label_desc': test_dset.label_desc,\n",
    "                'label_id': test_dset.targets\n",
    "            })\n",
    "            df1.to_csv(os.path.join(args.output, 'test_GT.csv'), index=False)\n",
    "            # save the average predictions of the test slides\n",
    "            df2 = pd.DataFrame({\n",
    "                'wsi_id': test_dset.slides,\n",
    "                'nonMSI_prob': test_slide_avg[:, 0],\n",
    "                'MSI_prob': test_slide_avg[:, 1]\n",
    "            })\n",
    "            df2.to_csv(os.path.join(args.output, 'test_pred_avg_AUC.csv'), index=False)\n",
    "            # save the max predictions of the test slides\n",
    "            df3 = pd.DataFrame({\n",
    "                'wsi_id': test_dset.slides,\n",
    "                'nonMSI_prob': test_slide_max[:, 0],\n",
    "                'MSI_prob': test_slide_max[:, 1]\n",
    "            })\n",
    "            df3.to_csv(os.path.join(args.output, 'test_pred_max_AUC.csv'), index=False)\n",
    "            # save the top10 predictions of the test slides\n",
    "            df4 = pd.DataFrame({\n",
    "                'wsi_id': test_dset.slides,\n",
    "                'nonMSI_prob': test_slide_avgt10[:, 0],\n",
    "                'MSI_prob': test_slide_avgt10[:, 1]\n",
    "            })\n",
    "            df4.to_csv(os.path.join(args.output, 'test_pred_t10_AUC.csv'), index=False)\n",
    "            # save original tile level predictions for test slides\n",
    "            df5 = pd.DataFrame({\n",
    "                'slideidx': test_dset.slideIDX,\n",
    "                'nonMSI_prob': test_probs[:, 0],\n",
    "                'MSI_prob': test_probs[:, 1],\n",
    "                'pred_tile': test_preds\n",
    "            })\n",
    "            df5.to_csv(os.path.join(args.output, 'test_tile_pred_AUC.csv'), index=False)\n",
    "            print('..............Test set done ...............')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_file = r\"E:\\Aamir Gulzar\\existing_approaches\\IDaRS_Fivecrop_4Folds\\MSI_vs_MSS_T1R10\\fold1\\best0\\test_GT.csv\"\n",
    "# pred_avg_file = r\"E:\\Aamir Gulzar\\existing_approaches\\IDaRS_Fivecrop_4Folds\\MSI_vs_MSS_T1R10\\fold1\\best0\\test_pred_avg_AUC.csv\"\n",
    "# pred_max_file = r\"E:\\Aamir Gulzar\\existing_approaches\\IDaRS_Fivecrop_4Folds\\MSI_vs_MSS_T1R10\\fold1\\best0\\test_pred_max_AUC.csv\"\n",
    "# pred_t10_file = r\"E:\\Aamir Gulzar\\existing_approaches\\IDaRS_Fivecrop_4Folds\\MSI_vs_MSS_T1R10\\fold1\\best0\\test_pred_t10_AUC.csv\"\n",
    "# # 1. Read the CSV files\n",
    "# df_gt = pd.read_csv(gt_file)  # columns: Case_ID, WSI_Id, n_tiles, label_desc, label_id\n",
    "# df_avg_pred = pd.read_csv(pred_avg_file)  # columns: wsi_id, non\n",
    "# df_max_pred = pd.read_csv(pred_max_file)  # columns: wsi_id, non\n",
    "# df_t10_pred = pd.read_csv(pred_t10_file)  # columns: wsi_id, non\n",
    "# # no need to merge already in the same order\n",
    "# # 3. Extract ground-truth labels and predicted probabilities\n",
    "# y_true = df_gt['label_id'].values\n",
    "# avg_pred_probs = df_avg_pred['MSI_prob'].values \n",
    "# max_pred_probs = df_max_pred['MSI_prob'].values \n",
    "# t10_pred_probs = df_t10_pred['MSI_prob'].values \n",
    "\n",
    "\n",
    "# avg_auc = roc_auc_score(y_true, avg_pred_probs)\n",
    "# max_auc = roc_auc_score(y_true, max_pred_probs)\n",
    "# t10_auc = roc_auc_score(y_true, t10_pred_probs)\n",
    "\n",
    "# print(f\"Average AUC = {avg_auc:.4f}\")\n",
    "# print(f\"Max AUC = {max_auc:.4f}\")\n",
    "# print(f\"Top10 AUC = {t10_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
